* 
* ==> Audit <==
* |-----------|-------------------------------------|----------|--------|---------|---------------------|---------------------|
|  Command  |                Args                 | Profile  |  User  | Version |     Start Time      |      End Time       |
|-----------|-------------------------------------|----------|--------|---------|---------------------|---------------------|
| start     |                                     | minikube | SOwusu | v1.32.0 | 26 Nov 23 20:13 CET | 26 Nov 23 20:15 CET |
| dashboard |                                     | minikube | SOwusu | v1.32.0 | 26 Nov 23 20:29 CET |                     |
| dashboard |                                     | minikube | SOwusu | v1.32.0 | 28 Nov 23 05:55 CET |                     |
| ip        |                                     | minikube | SOwusu | v1.32.0 | 28 Nov 23 07:31 CET | 28 Nov 23 07:31 CET |
| stop      |                                     | minikube | SOwusu | v1.32.0 | 28 Nov 23 15:27 CET | 28 Nov 23 15:27 CET |
| start     |                                     | minikube | SOwusu | v1.32.0 | 28 Nov 23 23:03 CET | 28 Nov 23 23:04 CET |
| addons    | enable metrics-server               | minikube | SOwusu | v1.32.0 | 28 Nov 23 23:04 CET | 28 Nov 23 23:04 CET |
| dashboard |                                     | minikube | SOwusu | v1.32.0 | 28 Nov 23 23:04 CET |                     |
| ip        |                                     | minikube | SOwusu | v1.32.0 | 28 Nov 23 23:35 CET | 28 Nov 23 23:35 CET |
| dashboard |                                     | minikube | SOwusu | v1.32.0 | 29 Nov 23 04:15 CET |                     |
| ip        |                                     | minikube | SOwusu | v1.32.0 | 29 Nov 23 05:17 CET | 29 Nov 23 05:17 CET |
| ip        |                                     | minikube | SOwusu | v1.32.0 | 29 Nov 23 05:53 CET | 29 Nov 23 05:53 CET |
| service   | esquire-microservice -n             | minikube | SOwusu | v1.32.0 | 29 Nov 23 07:21 CET |                     |
|           | default                             |          |        |         |                     |                     |
| service   | esquire-microservice                | minikube | SOwusu | v1.32.0 | 29 Nov 23 07:22 CET |                     |
| service   | esquire-service-dev                 | minikube | SOwusu | v1.32.0 | 29 Nov 23 07:22 CET |                     |
| service   | esquire-deployment-7755f7878f-zd7ck | minikube | SOwusu | v1.32.0 | 29 Nov 23 07:23 CET |                     |
| service   | esquire-service                     | minikube | SOwusu | v1.32.0 | 29 Nov 23 19:26 CET |                     |
| service   | esquire                             | minikube | SOwusu | v1.32.0 | 30 Nov 23 06:27 CET |                     |
| service   | esquire -n dev                      | minikube | SOwusu | v1.32.0 | 30 Nov 23 06:28 CET |                     |
| service   |                                     | minikube | SOwusu | v1.32.0 | 30 Nov 23 06:28 CET |                     |
| service   | --all                               | minikube | SOwusu | v1.32.0 | 30 Nov 23 06:28 CET |                     |
| service   | esquire                             | minikube | SOwusu | v1.32.0 | 30 Nov 23 06:37 CET |                     |
| service   | --all                               | minikube | SOwusu | v1.32.0 | 30 Nov 23 06:37 CET |                     |
|-----------|-------------------------------------|----------|--------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/11/28 23:03:23
Running on machine: HWN0291
Binary: Built with gc go1.21.4 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1128 23:03:23.178864   79556 out.go:296] Setting OutFile to fd 1 ...
I1128 23:03:23.179887   79556 out.go:348] isatty.IsTerminal(1) = true
I1128 23:03:23.179890   79556 out.go:309] Setting ErrFile to fd 2...
I1128 23:03:23.179893   79556 out.go:348] isatty.IsTerminal(2) = true
I1128 23:03:23.180020   79556 root.go:338] Updating PATH: /Users/SOwusu/.minikube/bin
W1128 23:03:23.180585   79556 root.go:314] Error reading config file at /Users/SOwusu/.minikube/config/config.json: open /Users/SOwusu/.minikube/config/config.json: no such file or directory
I1128 23:03:23.182531   79556 out.go:303] Setting JSON to false
I1128 23:03:23.225071   79556 start.go:128] hostinfo: {"hostname":"HWN0291","uptime":201028,"bootTime":1701007975,"procs":530,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.1.1","kernelVersion":"23.1.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"b823bd9a-a953-5a3c-bae3-dffaffaf5e7c"}
W1128 23:03:23.225168   79556 start.go:136] gopshost.Virtualization returned error: not implemented yet
I1128 23:03:23.229611   79556 out.go:177] 😄  minikube v1.32.0 on Darwin 14.1.1 (arm64)
W1128 23:03:23.241363   79556 preload.go:295] Failed to list preload files: open /Users/SOwusu/.minikube/cache/preloaded-tarball: no such file or directory
I1128 23:03:23.241515   79556 notify.go:220] Checking for updates...
I1128 23:03:23.243935   79556 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1128 23:03:23.244208   79556 driver.go:378] Setting default libvirt URI to qemu:///system
I1128 23:03:23.755441   79556 docker.go:122] docker version: linux-dev:Docker Desktop 4.24.0 (122432)
I1128 23:03:23.755764   79556 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1128 23:03:24.514372   79556 info.go:266] docker info: {ID:39c83200-6552-4c0b-8bc0-41806cf1c91d Containers:3 ContainersRunning:2 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:stargz DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:109 OomKillDisable:false NGoroutines:95 SystemTime:2023-11-28 22:03:24.500646468 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:6.4.16-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:11 MemTotal:8232550400 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:true ServerVersion:dev ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/SOwusu/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:/Users/SOwusu/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:/Users/SOwusu/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-dev] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/SOwusu/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-extension] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/SOwusu/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-init] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:/Users/SOwusu/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-sbom] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/SOwusu/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-scan] ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/SOwusu/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-scout] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
I1128 23:03:24.524981   79556 out.go:177] ✨  Using the docker driver based on existing profile
I1128 23:03:24.529960   79556 start.go:298] selected driver: docker
I1128 23:03:24.529983   79556 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1128 23:03:24.530045   79556 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:true Error:<nil> Reason: Fix:Install the official release of Docker Desktop (Minimum recommended version is 20.10.0, minimum supported version is 18.09.0, current version is dev) Doc: Version:}
I1128 23:03:24.536982   79556 out.go:177] 💨  For improved Docker Desktop performance, Install the official release of Docker Desktop (Minimum recommended version is 20.10.0, minimum supported version is 18.09.0, current version is dev)
I1128 23:03:24.542050   79556 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1128 23:03:24.655127   79556 info.go:266] docker info: {ID:39c83200-6552-4c0b-8bc0-41806cf1c91d Containers:3 ContainersRunning:2 ContainersPaused:0 ContainersStopped:1 Images:2 Driver:stargz DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:109 OomKillDisable:false NGoroutines:95 SystemTime:2023-11-28 22:03:24.641503259 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:9 KernelVersion:6.4.16-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:11 MemTotal:8232550400 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:true ServerVersion:dev ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/SOwusu/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:/Users/SOwusu/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.22.0-desktop.2] map[Name:dev Path:/Users/SOwusu/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-dev] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/SOwusu/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-extension] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:/Users/SOwusu/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-init] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.8] map[Name:sbom Path:/Users/SOwusu/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-sbom] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/SOwusu/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-scan] ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/SOwusu/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShadowedPaths:[/usr/local/lib/docker/cli-plugins/docker-scout] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.7]] Warnings:<nil>}}
W1128 23:03:24.655281   79556 out.go:239] ❗  docker is currently using the stargz storage driver, setting preload=false
I1128 23:03:24.655996   79556 cni.go:84] Creating CNI manager for ""
I1128 23:03:24.656022   79556 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1128 23:03:24.656030   79556 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1128 23:03:24.669893   79556 out.go:177] 👍  Starting control plane node minikube in cluster minikube
I1128 23:03:24.674466   79556 cache.go:121] Beginning downloading kic base image for docker with docker
I1128 23:03:24.678909   79556 out.go:177] 🚜  Pulling base image ...
I1128 23:03:24.686995   79556 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I1128 23:03:24.687155   79556 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1128 23:03:24.687247   79556 profile.go:148] Saving config to /Users/SOwusu/.minikube/profiles/minikube/config.json ...
I1128 23:03:24.688646   79556 cache.go:107] acquiring lock: {Name:mk8a34c7e9a5836500cf8fd4ff3818ef0c34ae7a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1128 23:03:24.688724   79556 cache.go:107] acquiring lock: {Name:mk0cfaed1f6b007fb637e62b872f26e4132732bd Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1128 23:03:24.688745   79556 cache.go:107] acquiring lock: {Name:mk0b3dd01eb99c8135386930364783936ae5bbc4 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1128 23:03:24.688764   79556 cache.go:107] acquiring lock: {Name:mk2eda76315e0e2bf953631b79bdd3ccc4f9f406 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1128 23:03:24.688844   79556 cache.go:107] acquiring lock: {Name:mk8bbc8d2dd63d2f7d7a7536a792bdaf6403750f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1128 23:03:24.688650   79556 cache.go:107] acquiring lock: {Name:mk760a87cec50d19f000056514ea318ce6e5b6c2 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1128 23:03:24.688880   79556 cache.go:107] acquiring lock: {Name:mk87b2ef1374fac19ad7def9dc629ad5bf983ce1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1128 23:03:24.688915   79556 cache.go:107] acquiring lock: {Name:mk494f790bc9c6ce3c2a3effbdb713d0713bb11c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1128 23:03:24.692158   79556 cache.go:115] /Users/SOwusu/.minikube/cache/images/arm64/gcr.io/k8s-minikube/storage-provisioner_v5 exists
I1128 23:03:24.692175   79556 cache.go:96] cache image "gcr.io/k8s-minikube/storage-provisioner:v5" -> "/Users/SOwusu/.minikube/cache/images/arm64/gcr.io/k8s-minikube/storage-provisioner_v5" took 3.546958ms
I1128 23:03:24.692273   79556 cache.go:115] /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/kube-apiserver_v1.28.3 exists
I1128 23:03:24.692274   79556 cache.go:115] /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/pause_3.9 exists
I1128 23:03:24.692280   79556 cache.go:115] /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/kube-scheduler_v1.28.3 exists
I1128 23:03:24.692284   79556 cache.go:115] /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/kube-controller-manager_v1.28.3 exists
I1128 23:03:24.692286   79556 cache.go:115] /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/etcd_3.5.9-0 exists
I1128 23:03:24.692282   79556 cache.go:96] cache image "registry.k8s.io/kube-apiserver:v1.28.3" -> "/Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/kube-apiserver_v1.28.3" took 3.670792ms
I1128 23:03:24.692286   79556 cache.go:96] cache image "registry.k8s.io/pause:3.9" -> "/Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/pause_3.9" took 3.586708ms
I1128 23:03:24.692298   79556 cache.go:96] cache image "registry.k8s.io/etcd:3.5.9-0" -> "/Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/etcd_3.5.9-0" took 3.467542ms
I1128 23:03:24.692298   79556 cache.go:96] cache image "registry.k8s.io/kube-controller-manager:v1.28.3" -> "/Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/kube-controller-manager_v1.28.3" took 3.695875ms
I1128 23:03:24.692300   79556 cache.go:96] cache image "registry.k8s.io/kube-scheduler:v1.28.3" -> "/Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/kube-scheduler_v1.28.3" took 3.57325ms
I1128 23:03:24.692341   79556 cache.go:115] /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/coredns/coredns_v1.10.1 exists
I1128 23:03:24.692351   79556 cache.go:115] /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/kube-proxy_v1.28.3 exists
I1128 23:03:24.692353   79556 cache.go:96] cache image "registry.k8s.io/coredns/coredns:v1.10.1" -> "/Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/coredns/coredns_v1.10.1" took 3.7055ms
I1128 23:03:24.692366   79556 cache.go:96] cache image "registry.k8s.io/kube-proxy:v1.28.3" -> "/Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/kube-proxy_v1.28.3" took 3.499291ms
I1128 23:03:24.692375   79556 cache.go:80] save to tar file gcr.io/k8s-minikube/storage-provisioner:v5 -> /Users/SOwusu/.minikube/cache/images/arm64/gcr.io/k8s-minikube/storage-provisioner_v5 succeeded
I1128 23:03:24.692380   79556 cache.go:80] save to tar file registry.k8s.io/etcd:3.5.9-0 -> /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/etcd_3.5.9-0 succeeded
I1128 23:03:24.692386   79556 cache.go:80] save to tar file registry.k8s.io/kube-apiserver:v1.28.3 -> /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/kube-apiserver_v1.28.3 succeeded
I1128 23:03:24.692387   79556 cache.go:80] save to tar file registry.k8s.io/kube-controller-manager:v1.28.3 -> /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/kube-controller-manager_v1.28.3 succeeded
I1128 23:03:24.692388   79556 cache.go:80] save to tar file registry.k8s.io/pause:3.9 -> /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/pause_3.9 succeeded
I1128 23:03:24.692389   79556 cache.go:80] save to tar file registry.k8s.io/kube-proxy:v1.28.3 -> /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/kube-proxy_v1.28.3 succeeded
I1128 23:03:24.692392   79556 cache.go:80] save to tar file registry.k8s.io/coredns/coredns:v1.10.1 -> /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/coredns/coredns_v1.10.1 succeeded
I1128 23:03:24.692394   79556 cache.go:80] save to tar file registry.k8s.io/kube-scheduler:v1.28.3 -> /Users/SOwusu/.minikube/cache/images/arm64/registry.k8s.io/kube-scheduler_v1.28.3 succeeded
I1128 23:03:24.692401   79556 cache.go:87] Successfully saved all images to host disk.
I1128 23:03:24.825104   79556 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 to local cache
I1128 23:03:24.825758   79556 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory
I1128 23:03:24.825811   79556 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local cache directory, skipping pull
I1128 23:03:24.825968   79556 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in cache, skipping pull
I1128 23:03:24.825984   79556 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 as a tarball
I1128 23:03:24.825986   79556 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 from local cache
I1128 23:03:32.364069   79556 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 from cached tarball
I1128 23:03:32.364175   79556 cache.go:194] Successfully downloaded all kic artifacts
I1128 23:03:32.364373   79556 start.go:365] acquiring machines lock for minikube: {Name:mk19ec2b8426ca50c35dfc98fabe1dbfd51657c8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1128 23:03:32.366968   79556 start.go:369] acquired machines lock for "minikube" in 2.548791ms
I1128 23:03:32.367073   79556 start.go:96] Skipping create...Using existing machine configuration
I1128 23:03:32.367081   79556 fix.go:54] fixHost starting: 
I1128 23:03:32.368083   79556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1128 23:03:32.396311   79556 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1128 23:03:32.396425   79556 fix.go:128] unexpected machine state, will restart: <nil>
I1128 23:03:32.402789   79556 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I1128 23:03:32.411753   79556 cli_runner.go:164] Run: docker start minikube
I1128 23:03:33.021148   79556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1128 23:03:33.042711   79556 kic.go:430] container "minikube" state is running.
I1128 23:03:33.044590   79556 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1128 23:03:33.067834   79556 profile.go:148] Saving config to /Users/SOwusu/.minikube/profiles/minikube/config.json ...
I1128 23:03:33.068877   79556 machine.go:88] provisioning docker machine ...
I1128 23:03:33.068913   79556 ubuntu.go:169] provisioning hostname "minikube"
I1128 23:03:33.068992   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:03:33.088411   79556 main.go:141] libmachine: Using SSH client type: native
I1128 23:03:33.089255   79556 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1010cef80] 0x1010d16f0 <nil>  [] 0s} 127.0.0.1 54278 <nil> <nil>}
I1128 23:03:33.089262   79556 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1128 23:03:33.105716   79556 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1128 23:03:36.251041   79556 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1128 23:03:36.251203   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:03:36.291590   79556 main.go:141] libmachine: Using SSH client type: native
I1128 23:03:36.291927   79556 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1010cef80] 0x1010d16f0 <nil>  [] 0s} 127.0.0.1 54278 <nil> <nil>}
I1128 23:03:36.291933   79556 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1128 23:03:36.408810   79556 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1128 23:03:36.408828   79556 ubuntu.go:175] set auth options {CertDir:/Users/SOwusu/.minikube CaCertPath:/Users/SOwusu/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/SOwusu/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/SOwusu/.minikube/machines/server.pem ServerKeyPath:/Users/SOwusu/.minikube/machines/server-key.pem ClientKeyPath:/Users/SOwusu/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/SOwusu/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/SOwusu/.minikube}
I1128 23:03:36.408859   79556 ubuntu.go:177] setting up certificates
I1128 23:03:36.408871   79556 provision.go:83] configureAuth start
I1128 23:03:36.408984   79556 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1128 23:03:36.433783   79556 provision.go:138] copyHostCerts
I1128 23:03:36.434251   79556 exec_runner.go:144] found /Users/SOwusu/.minikube/ca.pem, removing ...
I1128 23:03:36.434269   79556 exec_runner.go:203] rm: /Users/SOwusu/.minikube/ca.pem
I1128 23:03:36.434677   79556 exec_runner.go:151] cp: /Users/SOwusu/.minikube/certs/ca.pem --> /Users/SOwusu/.minikube/ca.pem (1078 bytes)
I1128 23:03:36.435585   79556 exec_runner.go:144] found /Users/SOwusu/.minikube/cert.pem, removing ...
I1128 23:03:36.435588   79556 exec_runner.go:203] rm: /Users/SOwusu/.minikube/cert.pem
I1128 23:03:36.436938   79556 exec_runner.go:151] cp: /Users/SOwusu/.minikube/certs/cert.pem --> /Users/SOwusu/.minikube/cert.pem (1119 bytes)
I1128 23:03:36.437721   79556 exec_runner.go:144] found /Users/SOwusu/.minikube/key.pem, removing ...
I1128 23:03:36.437724   79556 exec_runner.go:203] rm: /Users/SOwusu/.minikube/key.pem
I1128 23:03:36.438565   79556 exec_runner.go:151] cp: /Users/SOwusu/.minikube/certs/key.pem --> /Users/SOwusu/.minikube/key.pem (1679 bytes)
I1128 23:03:36.438996   79556 provision.go:112] generating server cert: /Users/SOwusu/.minikube/machines/server.pem ca-key=/Users/SOwusu/.minikube/certs/ca.pem private-key=/Users/SOwusu/.minikube/certs/ca-key.pem org=SOwusu.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1128 23:03:36.535063   79556 provision.go:172] copyRemoteCerts
I1128 23:03:36.535191   79556 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1128 23:03:36.535224   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:03:36.564768   79556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54278 SSHKeyPath:/Users/SOwusu/.minikube/machines/minikube/id_rsa Username:docker}
I1128 23:03:36.653801   79556 ssh_runner.go:362] scp /Users/SOwusu/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1128 23:03:36.683149   79556 ssh_runner.go:362] scp /Users/SOwusu/.minikube/machines/server.pem --> /etc/docker/server.pem (1200 bytes)
I1128 23:03:36.711211   79556 ssh_runner.go:362] scp /Users/SOwusu/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1128 23:03:36.730683   79556 provision.go:86] duration metric: configureAuth took 321.801041ms
I1128 23:03:36.730692   79556 ubuntu.go:193] setting minikube options for container-runtime
I1128 23:03:36.731115   79556 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1128 23:03:36.731179   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:03:36.751164   79556 main.go:141] libmachine: Using SSH client type: native
I1128 23:03:36.751515   79556 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1010cef80] 0x1010d16f0 <nil>  [] 0s} 127.0.0.1 54278 <nil> <nil>}
I1128 23:03:36.751520   79556 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1128 23:03:36.875601   79556 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1128 23:03:36.875615   79556 ubuntu.go:71] root file system type: overlay
I1128 23:03:36.875784   79556 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1128 23:03:36.875931   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:03:36.900234   79556 main.go:141] libmachine: Using SSH client type: native
I1128 23:03:36.900602   79556 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1010cef80] 0x1010d16f0 <nil>  [] 0s} 127.0.0.1 54278 <nil> <nil>}
I1128 23:03:36.900636   79556 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1128 23:03:37.034703   79556 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1128 23:03:37.034865   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:03:37.059485   79556 main.go:141] libmachine: Using SSH client type: native
I1128 23:03:37.059820   79556 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1010cef80] 0x1010d16f0 <nil>  [] 0s} 127.0.0.1 54278 <nil> <nil>}
I1128 23:03:37.059829   79556 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1128 23:03:37.196005   79556 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1128 23:03:37.196016   79556 machine.go:91] provisioned docker machine in 4.127107833s
I1128 23:03:37.196022   79556 start.go:300] post-start starting for "minikube" (driver="docker")
I1128 23:03:37.196040   79556 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1128 23:03:37.196139   79556 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1128 23:03:37.196180   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:03:37.214444   79556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54278 SSHKeyPath:/Users/SOwusu/.minikube/machines/minikube/id_rsa Username:docker}
I1128 23:03:37.312669   79556 ssh_runner.go:195] Run: cat /etc/os-release
I1128 23:03:37.317116   79556 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1128 23:03:37.317150   79556 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1128 23:03:37.317161   79556 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1128 23:03:37.317167   79556 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I1128 23:03:37.317178   79556 filesync.go:126] Scanning /Users/SOwusu/.minikube/addons for local assets ...
I1128 23:03:37.317669   79556 filesync.go:126] Scanning /Users/SOwusu/.minikube/files for local assets ...
I1128 23:03:37.317836   79556 start.go:303] post-start completed in 121.808333ms
I1128 23:03:37.317997   79556 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1128 23:03:37.318086   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:03:37.342055   79556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54278 SSHKeyPath:/Users/SOwusu/.minikube/machines/minikube/id_rsa Username:docker}
I1128 23:03:37.457732   79556 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1128 23:03:37.462108   79556 fix.go:56] fixHost completed within 5.094979583s
I1128 23:03:37.462142   79556 start.go:83] releasing machines lock for "minikube", held for 5.09512025s
I1128 23:03:37.462356   79556 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1128 23:03:37.490217   79556 ssh_runner.go:195] Run: cat /version.json
I1128 23:03:37.490315   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:03:37.490588   79556 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1128 23:03:37.491240   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:03:37.516306   79556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54278 SSHKeyPath:/Users/SOwusu/.minikube/machines/minikube/id_rsa Username:docker}
I1128 23:03:37.519934   79556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54278 SSHKeyPath:/Users/SOwusu/.minikube/machines/minikube/id_rsa Username:docker}
I1128 23:03:37.614435   79556 ssh_runner.go:195] Run: systemctl --version
I1128 23:03:38.916302   79556 ssh_runner.go:235] Completed: systemctl --version: (1.301833167s)
I1128 23:03:38.916389   79556 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.425762459s)
I1128 23:03:38.916819   79556 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1128 23:03:38.923396   79556 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1128 23:03:38.943104   79556 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1128 23:03:38.943570   79556 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1128 23:03:38.951199   79556 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1128 23:03:38.951489   79556 start.go:472] detecting cgroup driver to use...
I1128 23:03:38.951520   79556 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1128 23:03:38.953408   79556 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1128 23:03:38.966606   79556 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1128 23:03:38.975017   79556 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1128 23:03:38.982756   79556 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1128 23:03:38.983055   79556 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1128 23:03:38.991263   79556 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1128 23:03:38.999858   79556 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1128 23:03:39.007550   79556 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1128 23:03:39.014985   79556 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1128 23:03:39.022230   79556 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1128 23:03:39.033770   79556 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1128 23:03:39.045504   79556 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1128 23:03:39.054386   79556 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1128 23:03:39.112309   79556 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1128 23:03:39.189346   79556 start.go:472] detecting cgroup driver to use...
I1128 23:03:39.189370   79556 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1128 23:03:39.190281   79556 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1128 23:03:39.202415   79556 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1128 23:03:39.204623   79556 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1128 23:03:39.216191   79556 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1128 23:03:39.232588   79556 ssh_runner.go:195] Run: which cri-dockerd
I1128 23:03:39.237449   79556 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1128 23:03:39.245717   79556 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1128 23:03:39.261239   79556 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1128 23:03:39.327257   79556 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1128 23:03:39.424375   79556 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1128 23:03:39.425049   79556 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1128 23:03:39.438571   79556 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1128 23:03:39.487646   79556 ssh_runner.go:195] Run: sudo systemctl restart docker
I1128 23:03:39.712040   79556 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1128 23:03:39.758743   79556 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1128 23:03:39.805982   79556 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1128 23:03:39.850561   79556 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1128 23:03:39.900520   79556 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1128 23:03:39.921458   79556 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1128 23:03:39.965678   79556 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1128 23:03:40.123479   79556 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1128 23:03:40.124186   79556 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1128 23:03:40.127384   79556 start.go:540] Will wait 60s for crictl version
I1128 23:03:40.127455   79556 ssh_runner.go:195] Run: which crictl
I1128 23:03:40.130940   79556 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1128 23:03:40.234234   79556 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1128 23:03:40.234433   79556 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1128 23:03:40.283170   79556 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1128 23:03:40.312063   79556 out.go:204] 🐳  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1128 23:03:40.312592   79556 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1128 23:03:40.455541   79556 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1128 23:03:40.455706   79556 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1128 23:03:40.462390   79556 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1128 23:03:40.476912   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1128 23:03:40.514869   79556 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1128 23:03:40.515384   79556 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1128 23:03:40.556699   79556 docker.go:671] Got preloaded images: -- stdout --
quay.io/argoproj/argocd:v2.9.2
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
ghcr.io/dexidp/dex:v2.37.0
redis:7.0.11-alpine
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5
gcr.io/heptio-images/ks-guestbook-demo:0.2

-- /stdout --
I1128 23:03:40.557276   79556 cache_images.go:84] Images are preloaded, skipping loading
I1128 23:03:40.557388   79556 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1128 23:03:40.798662   79556 cni.go:84] Creating CNI manager for ""
I1128 23:03:40.798680   79556 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1128 23:03:40.799224   79556 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1128 23:03:40.799249   79556 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1128 23:03:40.799892   79556 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1128 23:03:40.800500   79556 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1128 23:03:40.800590   79556 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1128 23:03:40.809504   79556 binaries.go:44] Found k8s binaries, skipping transfer
I1128 23:03:40.809609   79556 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1128 23:03:40.816538   79556 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1128 23:03:40.829778   79556 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1128 23:03:40.847145   79556 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1128 23:03:40.864339   79556 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1128 23:03:40.868188   79556 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1128 23:03:40.878126   79556 certs.go:56] Setting up /Users/SOwusu/.minikube/profiles/minikube for IP: 192.168.49.2
I1128 23:03:40.878516   79556 certs.go:190] acquiring lock for shared ca certs: {Name:mkc9127c740116bf35a13b8badd6d3925f3eacd0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1128 23:03:40.880296   79556 certs.go:199] skipping minikubeCA CA generation: /Users/SOwusu/.minikube/ca.key
I1128 23:03:40.880880   79556 certs.go:199] skipping proxyClientCA CA generation: /Users/SOwusu/.minikube/proxy-client-ca.key
I1128 23:03:40.881716   79556 certs.go:315] skipping minikube-user signed cert generation: /Users/SOwusu/.minikube/profiles/minikube/client.key
I1128 23:03:40.882084   79556 certs.go:315] skipping minikube signed cert generation: /Users/SOwusu/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I1128 23:03:40.882460   79556 certs.go:315] skipping aggregator signed cert generation: /Users/SOwusu/.minikube/profiles/minikube/proxy-client.key
I1128 23:03:40.883255   79556 certs.go:437] found cert: /Users/SOwusu/.minikube/certs/Users/SOwusu/.minikube/certs/ca-key.pem (1679 bytes)
I1128 23:03:40.883292   79556 certs.go:437] found cert: /Users/SOwusu/.minikube/certs/Users/SOwusu/.minikube/certs/ca.pem (1078 bytes)
I1128 23:03:40.883330   79556 certs.go:437] found cert: /Users/SOwusu/.minikube/certs/Users/SOwusu/.minikube/certs/cert.pem (1119 bytes)
I1128 23:03:40.883360   79556 certs.go:437] found cert: /Users/SOwusu/.minikube/certs/Users/SOwusu/.minikube/certs/key.pem (1679 bytes)
I1128 23:03:40.890557   79556 ssh_runner.go:362] scp /Users/SOwusu/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1128 23:03:40.907272   79556 ssh_runner.go:362] scp /Users/SOwusu/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1128 23:03:40.924033   79556 ssh_runner.go:362] scp /Users/SOwusu/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1128 23:03:40.941116   79556 ssh_runner.go:362] scp /Users/SOwusu/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1128 23:03:40.958537   79556 ssh_runner.go:362] scp /Users/SOwusu/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1128 23:03:40.974931   79556 ssh_runner.go:362] scp /Users/SOwusu/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1128 23:03:40.990692   79556 ssh_runner.go:362] scp /Users/SOwusu/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1128 23:03:41.009889   79556 ssh_runner.go:362] scp /Users/SOwusu/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1128 23:03:41.025390   79556 ssh_runner.go:362] scp /Users/SOwusu/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1128 23:03:41.041769   79556 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1128 23:03:41.055141   79556 ssh_runner.go:195] Run: openssl version
I1128 23:03:41.063375   79556 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1128 23:03:41.071999   79556 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1128 23:03:41.074713   79556 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov 26 19:15 /usr/share/ca-certificates/minikubeCA.pem
I1128 23:03:41.074788   79556 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1128 23:03:41.079536   79556 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1128 23:03:41.086400   79556 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1128 23:03:41.089913   79556 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1128 23:03:41.096810   79556 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1128 23:03:41.102042   79556 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1128 23:03:41.108789   79556 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1128 23:03:41.113828   79556 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1128 23:03:41.119238   79556 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1128 23:03:41.124858   79556 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1128 23:03:41.124975   79556 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1128 23:03:41.140675   79556 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1128 23:03:41.147635   79556 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1128 23:03:41.148029   79556 kubeadm.go:636] restartCluster start
I1128 23:03:41.148237   79556 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1128 23:03:41.154304   79556 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1128 23:03:41.154418   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1128 23:03:41.186696   79556 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /Users/SOwusu/.kube/config
I1128 23:03:41.186793   79556 kubeconfig.go:146] "minikube" context is missing from /Users/SOwusu/.kube/config - will repair!
I1128 23:03:41.187403   79556 lock.go:35] WriteFile acquiring /Users/SOwusu/.kube/config: {Name:mk36bde0dbd4650d45475271d46b001a260ce81b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1128 23:03:41.195945   79556 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1128 23:03:41.203196   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:41.203263   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:41.211521   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:41.211529   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:41.211597   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:41.218649   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:41.720316   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:41.721562   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:41.752337   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:42.221087   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:42.221228   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:42.244870   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:42.719496   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:42.719679   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:42.746896   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:43.219689   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:43.219947   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:43.251904   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:43.720079   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:43.720613   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:43.744840   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:44.219487   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:44.219755   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:44.255276   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:44.719551   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:44.719705   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:44.741466   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:45.220402   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:45.220604   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:45.242707   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:45.719056   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:45.719928   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:45.743243   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:46.220714   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:46.220907   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:46.245681   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:46.719831   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:46.719931   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:46.729153   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:47.220072   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:47.220155   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:47.262901   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:47.721325   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:47.721770   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:47.752165   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:48.220056   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:48.221898   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:48.261300   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:48.719905   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:48.720233   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:48.748773   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:49.219922   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:49.220091   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:49.231075   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:49.724297   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:49.724528   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:49.743440   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:50.220832   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:50.221162   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:50.250446   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:50.719964   79556 api_server.go:166] Checking apiserver status ...
I1128 23:03:50.720389   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1128 23:03:50.751231   79556 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1128 23:03:51.204608   79556 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I1128 23:03:51.204691   79556 kubeadm.go:1128] stopping kube-system containers ...
I1128 23:03:51.204850   79556 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1128 23:03:51.251661   79556 docker.go:469] Stopping containers: [6dbf27ac7321 ff3ff45717f3 8d6662125535 320fd4aad9a0 1c3bae8408f7 01107bd1cc65 478cf8d92562 ab424460edc4 aa947aca4bcb 398aca05e9a8 58102dca432e e899cd541d84 49387cf95f68 e3e17b49b903 32e13d34d4c7]
I1128 23:03:51.251742   79556 ssh_runner.go:195] Run: docker stop 6dbf27ac7321 ff3ff45717f3 8d6662125535 320fd4aad9a0 1c3bae8408f7 01107bd1cc65 478cf8d92562 ab424460edc4 aa947aca4bcb 398aca05e9a8 58102dca432e e899cd541d84 49387cf95f68 e3e17b49b903 32e13d34d4c7
I1128 23:03:51.271366   79556 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1128 23:03:51.282305   79556 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1128 23:03:51.289734   79556 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Nov 26 19:15 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Nov 26 19:15 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Nov 26 19:15 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5604 Nov 26 19:15 /etc/kubernetes/scheduler.conf

I1128 23:03:51.289869   79556 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1128 23:03:51.296545   79556 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1128 23:03:51.303368   79556 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1128 23:03:51.323522   79556 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1128 23:03:51.323603   79556 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1128 23:03:51.330769   79556 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1128 23:03:51.336818   79556 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1128 23:03:51.336876   79556 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1128 23:03:51.342947   79556 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1128 23:03:51.349833   79556 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1128 23:03:51.349838   79556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1128 23:03:51.501720   79556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1128 23:03:51.988139   79556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1128 23:03:52.087368   79556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1128 23:03:52.121842   79556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1128 23:03:52.161105   79556 api_server.go:52] waiting for apiserver process to appear ...
I1128 23:03:52.161214   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 23:03:52.172602   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 23:03:52.688952   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 23:03:53.188515   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 23:03:53.686692   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 23:03:53.709479   79556 api_server.go:72] duration metric: took 1.548367375s to wait for apiserver process to appear ...
I1128 23:03:53.709564   79556 api_server.go:88] waiting for apiserver healthz status ...
I1128 23:03:53.710369   79556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54282/healthz ...
I1128 23:03:53.725200   79556 api_server.go:269] stopped: https://127.0.0.1:54282/healthz: Get "https://127.0.0.1:54282/healthz": EOF
I1128 23:03:53.725235   79556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54282/healthz ...
I1128 23:03:53.733905   79556 api_server.go:269] stopped: https://127.0.0.1:54282/healthz: Get "https://127.0.0.1:54282/healthz": EOF
I1128 23:03:54.235040   79556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54282/healthz ...
I1128 23:03:54.247125   79556 api_server.go:269] stopped: https://127.0.0.1:54282/healthz: Get "https://127.0.0.1:54282/healthz": EOF
I1128 23:03:54.734380   79556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54282/healthz ...
I1128 23:03:57.400146   79556 api_server.go:279] https://127.0.0.1:54282/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1128 23:03:57.400183   79556 api_server.go:103] status: https://127.0.0.1:54282/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1128 23:03:57.400196   79556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54282/healthz ...
I1128 23:03:57.419942   79556 api_server.go:279] https://127.0.0.1:54282/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1128 23:03:57.419971   79556 api_server.go:103] status: https://127.0.0.1:54282/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[-]poststarthook/crd-informer-synced failed: reason withheld
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1128 23:03:57.735136   79556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54282/healthz ...
I1128 23:03:57.748272   79556 api_server.go:279] https://127.0.0.1:54282/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1128 23:03:57.748302   79556 api_server.go:103] status: https://127.0.0.1:54282/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1128 23:03:58.235092   79556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54282/healthz ...
I1128 23:03:58.268166   79556 api_server.go:279] https://127.0.0.1:54282/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1128 23:03:58.268190   79556 api_server.go:103] status: https://127.0.0.1:54282/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1128 23:03:58.734375   79556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54282/healthz ...
I1128 23:03:58.799744   79556 api_server.go:279] https://127.0.0.1:54282/healthz returned 200:
ok
I1128 23:03:58.822671   79556 api_server.go:141] control plane version: v1.28.3
I1128 23:03:58.822684   79556 api_server.go:131] duration metric: took 5.113081708s to wait for apiserver health ...
I1128 23:03:58.822698   79556 cni.go:84] Creating CNI manager for ""
I1128 23:03:58.822709   79556 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1128 23:03:58.830403   79556 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I1128 23:03:58.840351   79556 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1128 23:03:58.919386   79556 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1128 23:03:59.099824   79556 system_pods.go:43] waiting for kube-system pods to appear ...
I1128 23:03:59.125918   79556 system_pods.go:59] 7 kube-system pods found
I1128 23:03:59.125961   79556 system_pods.go:61] "coredns-5dd5756b68-2hhgn" [e3345cf9-61ae-4885-9db3-8d75ded21337] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1128 23:03:59.126007   79556 system_pods.go:61] "etcd-minikube" [c1f7fa7f-a506-4403-97af-fd0dee9ef177] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1128 23:03:59.126014   79556 system_pods.go:61] "kube-apiserver-minikube" [108aad7c-0e25-470b-bc1a-da8f3451c2d4] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1128 23:03:59.126018   79556 system_pods.go:61] "kube-controller-manager-minikube" [44825940-15ce-4622-9862-995dca481539] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1128 23:03:59.126021   79556 system_pods.go:61] "kube-proxy-ldvgc" [26e99831-090d-497c-8dd7-55858eaa4aa4] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1128 23:03:59.126023   79556 system_pods.go:61] "kube-scheduler-minikube" [2adf0cbd-4ee7-4b23-9711-f479afa5c25c] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1128 23:03:59.126032   79556 system_pods.go:61] "storage-provisioner" [6f39aa62-f8c6-4b4c-8fc2-b0b6038f17d5] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1128 23:03:59.126038   79556 system_pods.go:74] duration metric: took 26.197625ms to wait for pod list to return data ...
I1128 23:03:59.126042   79556 node_conditions.go:102] verifying NodePressure condition ...
I1128 23:03:59.200043   79556 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I1128 23:03:59.200158   79556 node_conditions.go:123] node cpu capacity is 11
I1128 23:03:59.200175   79556 node_conditions.go:105] duration metric: took 74.128625ms to run NodePressure ...
I1128 23:03:59.200221   79556 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1128 23:04:00.717176   79556 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.516916792s)
I1128 23:04:00.717213   79556 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1128 23:04:00.820866   79556 ops.go:34] apiserver oom_adj: -16
I1128 23:04:00.820880   79556 kubeadm.go:640] restartCluster took 19.672714584s
I1128 23:04:00.820888   79556 kubeadm.go:406] StartCluster complete in 19.695904583s
I1128 23:04:00.820910   79556 settings.go:142] acquiring lock: {Name:mk21688361c890041d109c16c6670e3fb83e585f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1128 23:04:00.822101   79556 settings.go:150] Updating kubeconfig:  /Users/SOwusu/.kube/config
I1128 23:04:00.823406   79556 lock.go:35] WriteFile acquiring /Users/SOwusu/.kube/config: {Name:mk36bde0dbd4650d45475271d46b001a260ce81b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1128 23:04:00.829868   79556 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1128 23:04:00.830387   79556 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1128 23:04:00.830188   79556 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I1128 23:04:00.830569   79556 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1128 23:04:00.830584   79556 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1128 23:04:00.830575   79556 addons.go:69] Setting dashboard=true in profile "minikube"
I1128 23:04:00.830751   79556 addons.go:231] Setting addon dashboard=true in "minikube"
W1128 23:04:00.830754   79556 addons.go:240] addon dashboard should already be in state true
I1128 23:04:00.830769   79556 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1128 23:04:00.830812   79556 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W1128 23:04:00.830815   79556 addons.go:240] addon storage-provisioner should already be in state true
I1128 23:04:00.831358   79556 host.go:66] Checking if "minikube" exists ...
I1128 23:04:00.831373   79556 host.go:66] Checking if "minikube" exists ...
I1128 23:04:00.831377   79556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1128 23:04:00.831619   79556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1128 23:04:00.832040   79556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1128 23:04:00.846680   79556 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1128 23:04:00.847250   79556 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1128 23:04:00.857812   79556 out.go:177] 🔎  Verifying Kubernetes components...
I1128 23:04:00.872147   79556 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1128 23:04:00.961490   79556 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I1128 23:04:00.940494   79556 addons.go:231] Setting addon default-storageclass=true in "minikube"
I1128 23:04:00.956535   79556 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
W1128 23:04:00.961510   79556 addons.go:240] addon default-storageclass should already be in state true
I1128 23:04:00.961534   79556 host.go:66] Checking if "minikube" exists ...
I1128 23:04:00.971644   79556 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1128 23:04:00.965971   79556 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1128 23:04:00.971679   79556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1128 23:04:00.976703   79556 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1128 23:04:00.966434   79556 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1128 23:04:00.976710   79556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1128 23:04:00.976756   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:04:00.976775   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:04:01.038400   79556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54278 SSHKeyPath:/Users/SOwusu/.minikube/machines/minikube/id_rsa Username:docker}
I1128 23:04:01.038484   79556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54278 SSHKeyPath:/Users/SOwusu/.minikube/machines/minikube/id_rsa Username:docker}
I1128 23:04:01.038742   79556 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1128 23:04:01.038750   79556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1128 23:04:01.038815   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1128 23:04:01.067070   79556 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:54278 SSHKeyPath:/Users/SOwusu/.minikube/machines/minikube/id_rsa Username:docker}
I1128 23:04:01.413750   79556 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1128 23:04:01.413770   79556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1128 23:04:01.415253   79556 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1128 23:04:01.417717   79556 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1128 23:04:01.519022   79556 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1128 23:04:01.519038   79556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1128 23:04:01.616129   79556 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1128 23:04:01.616145   79556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1128 23:04:01.821371   79556 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1128 23:04:01.821384   79556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I1128 23:04:02.099656   79556 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.269728s)
I1128 23:04:02.099833   79556 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1.227657792s)
I1128 23:04:02.099999   79556 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1128 23:04:02.100024   79556 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1128 23:04:02.107763   79556 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I1128 23:04:02.107801   79556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1128 23:04:02.128706   79556 api_server.go:52] waiting for apiserver process to appear ...
I1128 23:04:02.128888   79556 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1128 23:04:02.297406   79556 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1128 23:04:02.297423   79556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1128 23:04:02.422348   79556 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1128 23:04:02.422369   79556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1128 23:04:02.611692   79556 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1128 23:04:02.611708   79556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1128 23:04:02.709300   79556 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1128 23:04:02.709335   79556 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1128 23:04:02.810533   79556 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1128 23:04:03.813839   79556 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.398545417s)
I1128 23:04:04.032804   79556 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.615052958s)
I1128 23:04:04.032843   79556 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.903933959s)
I1128 23:04:04.032851   79556 api_server.go:72] duration metric: took 3.185445667s to wait for apiserver process to appear ...
I1128 23:04:04.032853   79556 api_server.go:88] waiting for apiserver healthz status ...
I1128 23:04:04.032860   79556 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:54282/healthz ...
I1128 23:04:04.048950   79556 api_server.go:279] https://127.0.0.1:54282/healthz returned 200:
ok
I1128 23:04:04.054193   79556 api_server.go:141] control plane version: v1.28.3
I1128 23:04:04.054212   79556 api_server.go:131] duration metric: took 21.354292ms to wait for apiserver health ...
I1128 23:04:04.054224   79556 system_pods.go:43] waiting for kube-system pods to appear ...
I1128 23:04:04.102858   79556 system_pods.go:59] 7 kube-system pods found
I1128 23:04:04.102868   79556 system_pods.go:61] "coredns-5dd5756b68-2hhgn" [e3345cf9-61ae-4885-9db3-8d75ded21337] Running
I1128 23:04:04.102871   79556 system_pods.go:61] "etcd-minikube" [c1f7fa7f-a506-4403-97af-fd0dee9ef177] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1128 23:04:04.102878   79556 system_pods.go:61] "kube-apiserver-minikube" [108aad7c-0e25-470b-bc1a-da8f3451c2d4] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1128 23:04:04.102885   79556 system_pods.go:61] "kube-controller-manager-minikube" [44825940-15ce-4622-9862-995dca481539] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1128 23:04:04.102886   79556 system_pods.go:61] "kube-proxy-ldvgc" [26e99831-090d-497c-8dd7-55858eaa4aa4] Running
I1128 23:04:04.102889   79556 system_pods.go:61] "kube-scheduler-minikube" [2adf0cbd-4ee7-4b23-9711-f479afa5c25c] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1128 23:04:04.102891   79556 system_pods.go:61] "storage-provisioner" [6f39aa62-f8c6-4b4c-8fc2-b0b6038f17d5] Running
I1128 23:04:04.102900   79556 system_pods.go:74] duration metric: took 48.672125ms to wait for pod list to return data ...
I1128 23:04:04.102905   79556 kubeadm.go:581] duration metric: took 3.255500042s to wait for : map[apiserver:true system_pods:true] ...
I1128 23:04:04.102913   79556 node_conditions.go:102] verifying NodePressure condition ...
I1128 23:04:04.105959   79556 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I1128 23:04:04.105966   79556 node_conditions.go:123] node cpu capacity is 11
I1128 23:04:04.105971   79556 node_conditions.go:105] duration metric: took 3.056209ms to run NodePressure ...
I1128 23:04:04.105976   79556 start.go:228] waiting for startup goroutines ...
I1128 23:04:04.257922   79556 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (1.4473545s)
I1128 23:04:04.267275   79556 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I1128 23:04:04.274120   79556 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner, dashboard
I1128 23:04:04.287325   79556 addons.go:502] enable addons completed in 3.457323375s: enabled=[default-storageclass storage-provisioner dashboard]
I1128 23:04:04.287348   79556 start.go:233] waiting for cluster config update ...
I1128 23:04:04.287359   79556 start.go:242] writing updated cluster config ...
I1128 23:04:04.289996   79556 ssh_runner.go:195] Run: rm -f paused
I1128 23:04:04.894152   79556 start.go:600] kubectl: 1.28.4, cluster: 1.28.3 (minor skew: 0)
I1128 23:04:04.901525   79556 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Nov 28 22:04:11 minikube cri-dockerd[1069]: time="2023-11-28T22:04:11Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.9.2: Status: Image is up to date for quay.io/argoproj/argocd:v2.9.2"
Nov 28 22:04:13 minikube cri-dockerd[1069]: time="2023-11-28T22:04:13Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.9.2: Status: Image is up to date for quay.io/argoproj/argocd:v2.9.2"
Nov 28 22:04:13 minikube dockerd[816]: time="2023-11-28T22:04:13.158138254Z" level=info msg="ignoring event" container=9dd3766d4013dc022f749f9a5a5cfa54daaae860725e3b8ba89e0febd7f020ce module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 22:04:14 minikube cri-dockerd[1069]: time="2023-11-28T22:04:14Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.9.2: Status: Image is up to date for quay.io/argoproj/argocd:v2.9.2"
Nov 28 22:04:16 minikube cri-dockerd[1069]: time="2023-11-28T22:04:16Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.9.2: Status: Image is up to date for quay.io/argoproj/argocd:v2.9.2"
Nov 28 22:04:17 minikube cri-dockerd[1069]: time="2023-11-28T22:04:17Z" level=info msg="Stop pulling image quay.io/argoproj/argocd:v2.9.2: Status: Image is up to date for quay.io/argoproj/argocd:v2.9.2"
Nov 28 22:04:19 minikube cri-dockerd[1069]: time="2023-11-28T22:04:19Z" level=info msg="Stop pulling image ghcr.io/dexidp/dex:v2.37.0: Status: Image is up to date for ghcr.io/dexidp/dex:v2.37.0"
Nov 28 22:04:22 minikube cri-dockerd[1069]: time="2023-11-28T22:04:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a274c6919b2552e95e721984940fd8909561a28fb15f9851dd47b19d9db89904/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 28 22:04:22 minikube dockerd[816]: time="2023-11-28T22:04:22.750861592Z" level=warning msg="reference for unknown type: " digest="sha256:ee4304963fb035239bb5c5e8c10f2f38ee80efc16ecbdb9feb7213c17ae2e86e" remote="registry.k8s.io/metrics-server/metrics-server@sha256:ee4304963fb035239bb5c5e8c10f2f38ee80efc16ecbdb9feb7213c17ae2e86e"
Nov 28 22:04:26 minikube cri-dockerd[1069]: time="2023-11-28T22:04:26Z" level=info msg="Stop pulling image registry.k8s.io/metrics-server/metrics-server:v0.6.4@sha256:ee4304963fb035239bb5c5e8c10f2f38ee80efc16ecbdb9feb7213c17ae2e86e: Status: Downloaded newer image for registry.k8s.io/metrics-server/metrics-server@sha256:ee4304963fb035239bb5c5e8c10f2f38ee80efc16ecbdb9feb7213c17ae2e86e"
Nov 28 22:04:31 minikube dockerd[816]: time="2023-11-28T22:04:31.432867013Z" level=info msg="ignoring event" container=07dc7ac00e31185cc7d6c07f4ae538e6dd7a59f08d958716218ec51e64ee1780 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 28 22:04:32 minikube dockerd[816]: time="2023-11-28T22:04:32.544154013Z" level=info msg="ignoring event" container=e50ec26784344d679f6ba6f0d518eab8c74f0db255e2fff1aa24d6a497dc3cf8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 02:58:02 minikube cri-dockerd[1069]: time="2023-11-29T02:58:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dc0a23c3711a86accc29fbceb3de32dc1e3df5c23044ea93ce0161528a58cb54/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 02:58:02 minikube cri-dockerd[1069]: time="2023-11-29T02:58:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/317fdd5bb3324b540c6b153cd22241d8205e63ee78f9a4e1e426ec14687e35b4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 02:58:02 minikube cri-dockerd[1069]: time="2023-11-29T02:58:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f27f37c74e4c2e01b2c1d67ab402d04cb99a6b058ac87594aacf04018279534c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 02:58:17 minikube cri-dockerd[1069]: time="2023-11-29T02:58:17Z" level=info msg="Pulling image zx8086/esquire-microservice:7: b7d791d3063c: Extracting [==================================================>]  35.27MB/35.27MB"
Nov 29 02:58:27 minikube cri-dockerd[1069]: time="2023-11-29T02:58:27Z" level=info msg="Pulling image zx8086/esquire-microservice:7: 9aa9338ac858: Extracting [==============================>                    ]  17.99MB/29.21MB"
Nov 29 02:58:33 minikube cri-dockerd[1069]: time="2023-11-29T02:58:33Z" level=info msg="Stop pulling image zx8086/esquire-microservice:7: Status: Downloaded newer image for zx8086/esquire-microservice:7"
Nov 29 02:58:35 minikube cri-dockerd[1069]: time="2023-11-29T02:58:35Z" level=info msg="Stop pulling image zx8086/esquire-microservice:7: Status: Image is up to date for zx8086/esquire-microservice:7"
Nov 29 02:58:36 minikube cri-dockerd[1069]: time="2023-11-29T02:58:36Z" level=info msg="Stop pulling image zx8086/esquire-microservice:7: Status: Image is up to date for zx8086/esquire-microservice:7"
Nov 29 03:06:32 minikube dockerd[816]: time="2023-11-29T03:06:32.796434678Z" level=info msg="ignoring event" container=186572ed7c409ddd897eda7d8659f1b7973ab0ed795858b0ed8ca21ed5c80985 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:06:32 minikube dockerd[816]: time="2023-11-29T03:06:32.796522719Z" level=info msg="ignoring event" container=aa5be0a7eeb76e6806e10bf219c8b015a0e4673b254713824e2b1b9c2957c464 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:06:32 minikube dockerd[816]: time="2023-11-29T03:06:32.796541928Z" level=info msg="ignoring event" container=d7e5fc372c0c0f01440792951cd0f882c4e1522a0594de86c852b1ce6ad8af30 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:06:33 minikube dockerd[816]: time="2023-11-29T03:06:33.280429886Z" level=info msg="ignoring event" container=f27f37c74e4c2e01b2c1d67ab402d04cb99a6b058ac87594aacf04018279534c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:06:33 minikube dockerd[816]: time="2023-11-29T03:06:33.287172011Z" level=info msg="ignoring event" container=317fdd5bb3324b540c6b153cd22241d8205e63ee78f9a4e1e426ec14687e35b4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:06:33 minikube dockerd[816]: time="2023-11-29T03:06:33.299801053Z" level=info msg="ignoring event" container=dc0a23c3711a86accc29fbceb3de32dc1e3df5c23044ea93ce0161528a58cb54 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:27:58 minikube cri-dockerd[1069]: time="2023-11-29T03:27:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e7e17dfb990d56520375c0173480d25224b90520911d99aa5f17ff939360125b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 03:27:58 minikube cri-dockerd[1069]: time="2023-11-29T03:27:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/20ac6a72e9a32a9c1cc36a7ecf7106327b7c5a9ca22ecea466dddaac427d1df5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 03:27:58 minikube cri-dockerd[1069]: time="2023-11-29T03:27:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6453542c3f9918dcca38d00d2cd351061ff20c52ba308d732e54ae0c9258c203/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 03:30:28 minikube dockerd[816]: time="2023-11-29T03:30:28.266609009Z" level=info msg="ignoring event" container=8d9988c9e0eca3373c59357f989655244f55a46bd81d926d45043e6834a6a9fd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:30:28 minikube dockerd[816]: time="2023-11-29T03:30:28.372851217Z" level=info msg="ignoring event" container=e434f4ecde28a905dfa1d117a83775d435f02b532154a38e0b571a3702191a36 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:31:41 minikube dockerd[816]: time="2023-11-29T03:31:41.681877876Z" level=info msg="ignoring event" container=6fe410797e7e2e1aaee10bfa708f15c2b620f171c9e4510d387e2a712705f269 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:31:41 minikube dockerd[816]: time="2023-11-29T03:31:41.893509834Z" level=info msg="ignoring event" container=20ac6a72e9a32a9c1cc36a7ecf7106327b7c5a9ca22ecea466dddaac427d1df5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:31:42 minikube cri-dockerd[1069]: time="2023-11-29T03:31:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7c2688e7db1c2a81c056b7d9ae389b33dcf7b7b66fc91dbd802b5fc6f67082ac/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 03:32:02 minikube dockerd[816]: time="2023-11-29T03:32:02.216481344Z" level=info msg="ignoring event" container=1bcd42dc35eadec228ee56fd7c3c0494d33e2abece85e61de58522768c73e3f8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:32:02 minikube dockerd[816]: time="2023-11-29T03:32:02.420804802Z" level=info msg="ignoring event" container=e7e17dfb990d56520375c0173480d25224b90520911d99aa5f17ff939360125b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:32:02 minikube cri-dockerd[1069]: time="2023-11-29T03:32:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4d4f0ae47531acd39f3042a680a9096c1e7cc43dbfbf1fa199b186cc163e5d4c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 03:33:19 minikube dockerd[816]: time="2023-11-29T03:33:19.918727088Z" level=info msg="ignoring event" container=609edb17fc969878c0bcec7c772f079dc556af2d0676c3fb8f8e93a909e531e5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:33:20 minikube dockerd[816]: time="2023-11-29T03:33:20.128191130Z" level=info msg="ignoring event" container=6453542c3f9918dcca38d00d2cd351061ff20c52ba308d732e54ae0c9258c203 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:33:20 minikube cri-dockerd[1069]: time="2023-11-29T03:33:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/46a2b0ad1919129113b4b2e1046cf4b8d595f3eb7133ae10e2cd8261e0f8e221/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 03:58:09 minikube dockerd[816]: time="2023-11-29T03:58:09.907465833Z" level=info msg="ignoring event" container=26e3ca46bba9b172674ca869d1d9912952b0b8996aecba1403c2ccf7a923be68 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:58:09 minikube dockerd[816]: time="2023-11-29T03:58:09.917958500Z" level=info msg="ignoring event" container=1791f11a406f1a1f4e967962dc673d9fcab81d5c517a02e13a1559988b317d09 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:58:10 minikube dockerd[816]: time="2023-11-29T03:58:10.299098833Z" level=info msg="ignoring event" container=4d4f0ae47531acd39f3042a680a9096c1e7cc43dbfbf1fa199b186cc163e5d4c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 03:58:10 minikube dockerd[816]: time="2023-11-29T03:58:10.317928667Z" level=info msg="ignoring event" container=7c2688e7db1c2a81c056b7d9ae389b33dcf7b7b66fc91dbd802b5fc6f67082ac module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 04:02:45 minikube cri-dockerd[1069]: time="2023-11-29T04:02:45Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f9f711db8d273e79bde457141e371cb35d6afbb9e648e2a8cd147697b0c80bd7/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 04:28:01 minikube dockerd[816]: time="2023-11-29T04:28:01.396311885Z" level=info msg="ignoring event" container=ab1ef8359cdab42fdfdedd4c53c344352280c0f952170f37b26000ac95afc80e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 04:34:59 minikube cri-dockerd[1069]: time="2023-11-29T04:34:59Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/59c1e681e09ec394ad395a0e421686dfad3dfd0eede6407dba9ad7680f606401/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 04:34:59 minikube dockerd[816]: time="2023-11-29T04:34:59.895551342Z" level=info msg="ignoring event" container=6ba13cb9574c5251b1a51968546302345b8faef81107f68b09033884b7ba71e9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 04:35:00 minikube dockerd[816]: time="2023-11-29T04:35:00.339957926Z" level=info msg="ignoring event" container=f9f711db8d273e79bde457141e371cb35d6afbb9e648e2a8cd147697b0c80bd7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 04:35:00 minikube cri-dockerd[1069]: time="2023-11-29T04:35:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0810ab94dcdfe11568cd5a34814e8cca7db8294e457453a59e1791883b35d91d/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 04:35:02 minikube dockerd[816]: time="2023-11-29T04:35:02.258224177Z" level=info msg="ignoring event" container=02ddf293b9343e3c4ed0d3ce23ab4d1924c9c6ef83aedfbf5bfb21c478cad5ad module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 04:35:02 minikube dockerd[816]: time="2023-11-29T04:35:02.674790510Z" level=info msg="ignoring event" container=46a2b0ad1919129113b4b2e1046cf4b8d595f3eb7133ae10e2cd8261e0f8e221 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 05:24:14 minikube cri-dockerd[1069]: time="2023-11-29T05:24:14Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f7703844bbb31bc52c2df66c3591d2c917ee9b1bb613e1e1b6f564f858cda8f9/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Nov 29 05:26:03 minikube dockerd[816]: time="2023-11-29T05:26:03.165392052Z" level=info msg="ignoring event" container=5db2084f091b0beec3d8feff454d9a4c23676f27e14a03cb1f345580c40ac848 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 29 05:26:03 minikube dockerd[816]: time="2023-11-29T05:26:03.920816385Z" level=info msg="ignoring event" container=f7703844bbb31bc52c2df66c3591d2c917ee9b1bb613e1e1b6f564f858cda8f9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 04:56:23 minikube dockerd[816]: time="2023-11-30T04:56:23.188189219Z" level=info msg="ignoring event" container=4b27cebc9e62336124a06ec17b34342795bc9d2bd24db0464950393c94c66a28 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 04:56:23 minikube dockerd[816]: time="2023-11-30T04:56:23.203184802Z" level=info msg="ignoring event" container=c8b825f126e55b108e94de42e0289c75a605ee1fc607f3a5fd59bdd9b681e4df module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 04:56:23 minikube dockerd[816]: time="2023-11-30T04:56:23.647696386Z" level=info msg="ignoring event" container=0810ab94dcdfe11568cd5a34814e8cca7db8294e457453a59e1791883b35d91d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 04:56:23 minikube dockerd[816]: time="2023-11-30T04:56:23.662752177Z" level=info msg="ignoring event" container=59c1e681e09ec394ad395a0e421686dfad3dfd0eede6407dba9ad7680f606401 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Nov 30 05:24:55 minikube cri-dockerd[1069]: time="2023-11-30T05:24:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9fffb949afd172a5cb3cb9264b7792cf8b9d0671bf9ad1ae34dd18c3c7f2341e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                               ATTEMPT             POD ID              POD
2f17b971b3e48       6ef8aca717697                                                                                                           13 minutes ago      Running             esquire-microservice               0                   9fffb949afd17       esquire-microservice-77565d7689-m6qbb
d209003fc1c7e       66749159455b3                                                                                                           25 hours ago        Running             storage-provisioner                4                   1d6781627eed5       storage-provisioner
346b4c217ab77       20b332c9a70d8                                                                                                           32 hours ago        Running             kubernetes-dashboard               2                   bfa32061db7e0       kubernetes-dashboard-8694d4445c-hbhzk
ab1ef8359cdab       66749159455b3                                                                                                           32 hours ago        Exited              storage-provisioner                3                   1d6781627eed5       storage-provisioner
1b2821193b9fb       registry.k8s.io/metrics-server/metrics-server@sha256:ee4304963fb035239bb5c5e8c10f2f38ee80efc16ecbdb9feb7213c17ae2e86e   32 hours ago        Running             metrics-server                     0                   a274c6919b255       metrics-server-7c66d45ddc-nhs9l
03a64d72fbaa4       ghcr.io/dexidp/dex@sha256:f579d00721b0d842328c43a562f50343c54b0048ef2d58d6b54e750c21fc7938                              32 hours ago        Running             dex                                1                   64ef8041d830a       argocd-dex-server-f7648d898-859z5
35e3bf16bb46a       quay.io/argoproj/argocd@sha256:8576d347f30fa4c56a0129d1c0a0f5ed1e75662f0499f1ed7e917c405fd909dc                         32 hours ago        Running             argocd-repo-server                 1                   27d6ec241eead       argocd-repo-server-8477fdffc7-28bll
d2395c957e187       quay.io/argoproj/argocd@sha256:8576d347f30fa4c56a0129d1c0a0f5ed1e75662f0499f1ed7e917c405fd909dc                         32 hours ago        Running             argocd-applicationset-controller   1                   3cdaab4d4b587       argocd-applicationset-controller-5bf97c679b-bs9wm
7e48be63a0ddc       quay.io/argoproj/argocd@sha256:8576d347f30fa4c56a0129d1c0a0f5ed1e75662f0499f1ed7e917c405fd909dc                         32 hours ago        Running             argocd-notifications-controller    1                   30a0a3e531780       argocd-notifications-controller-6cf7579685-d8lg7
9dd3766d4013d       quay.io/argoproj/argocd@sha256:8576d347f30fa4c56a0129d1c0a0f5ed1e75662f0499f1ed7e917c405fd909dc                         32 hours ago        Exited              copyutil                           1                   64ef8041d830a       argocd-dex-server-f7648d898-859z5
00176e0e23e85       quay.io/argoproj/argocd@sha256:8576d347f30fa4c56a0129d1c0a0f5ed1e75662f0499f1ed7e917c405fd909dc                         32 hours ago        Running             argocd-application-controller      1                   077507a06f306       argocd-application-controller-0
eaebf85d7df4d       quay.io/argoproj/argocd@sha256:8576d347f30fa4c56a0129d1c0a0f5ed1e75662f0499f1ed7e917c405fd909dc                         32 hours ago        Running             argocd-server                      1                   f010f87e04e72       argocd-server-7c7d77f474-lls92
379799780e581       redis@sha256:121bac949fb5f623b9fa0b4e4c9fb358ffd045966e754cfa3eb9963f3af2fe3b                                           32 hours ago        Running             redis                              1                   8caf1b0ceef29       argocd-redis-6976fc7dfc-jdwxc
f930bb2fa6975       364c6a1d0de8b                                                                                                           32 hours ago        Exited              copyutil                           1                   27d6ec241eead       argocd-repo-server-8477fdffc7-28bll
c2765329113c9       97e04611ad434                                                                                                           32 hours ago        Running             coredns                            1                   b9816407ff152       coredns-5dd5756b68-2hhgn
e8e4b318f740e       a422e0e982356                                                                                                           32 hours ago        Running             dashboard-metrics-scraper          1                   eabdb453970ae       dashboard-metrics-scraper-7fd5cb4ddc-glg62
575de83ad1d0f       a5dd5cdd6d3ef                                                                                                           32 hours ago        Running             kube-proxy                         1                   e0d70a37b3d36       kube-proxy-ldvgc
ad74500e50d3a       8276439b4f237                                                                                                           32 hours ago        Running             kube-controller-manager            1                   2b636816c724c       kube-controller-manager-minikube
9b5e2153e7fc4       9cdd6470f48c8                                                                                                           32 hours ago        Running             etcd                               1                   005809d1f0a47       etcd-minikube
06924824e1f0e       42a4e73724daa                                                                                                           32 hours ago        Running             kube-scheduler                     1                   3c3f1a847f3aa       kube-scheduler-minikube
5203fc224b161       537e9a59ee2fd                                                                                                           32 hours ago        Running             kube-apiserver                     1                   78eb2dceee767       kube-apiserver-minikube
f0b6c51110722       quay.io/argoproj/argocd@sha256:8576d347f30fa4c56a0129d1c0a0f5ed1e75662f0499f1ed7e917c405fd909dc                         3 days ago          Exited              argocd-repo-server                 0                   38b4f3afca5cc       argocd-repo-server-8477fdffc7-28bll
e534db092187f       ghcr.io/dexidp/dex@sha256:f579d00721b0d842328c43a562f50343c54b0048ef2d58d6b54e750c21fc7938                              3 days ago          Exited              dex                                0                   7bacc23f86450       argocd-dex-server-f7648d898-859z5
1c0e5f8702d63       quay.io/argoproj/argocd@sha256:8576d347f30fa4c56a0129d1c0a0f5ed1e75662f0499f1ed7e917c405fd909dc                         3 days ago          Exited              argocd-notifications-controller    0                   3ae3debbc6a8c       argocd-notifications-controller-6cf7579685-d8lg7
eeb6e18c17de9       quay.io/argoproj/argocd@sha256:8576d347f30fa4c56a0129d1c0a0f5ed1e75662f0499f1ed7e917c405fd909dc                         3 days ago          Exited              argocd-application-controller      0                   6b4e674a747b4       argocd-application-controller-0
3e547e3f10012       quay.io/argoproj/argocd@sha256:8576d347f30fa4c56a0129d1c0a0f5ed1e75662f0499f1ed7e917c405fd909dc                         3 days ago          Exited              argocd-server                      0                   985273a6cab3c       argocd-server-7c7d77f474-lls92
8773c4b67f039       redis@sha256:121bac949fb5f623b9fa0b4e4c9fb358ffd045966e754cfa3eb9963f3af2fe3b                                           3 days ago          Exited              redis                              0                   262f8e165704c       argocd-redis-6976fc7dfc-jdwxc
e430c385dea5e       quay.io/argoproj/argocd@sha256:8576d347f30fa4c56a0129d1c0a0f5ed1e75662f0499f1ed7e917c405fd909dc                         3 days ago          Exited              argocd-applicationset-controller   0                   8667b095803d4       argocd-applicationset-controller-5bf97c679b-bs9wm
34da2fccbae08       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c                    3 days ago          Exited              dashboard-metrics-scraper          0                   49015ffd64e73       dashboard-metrics-scraper-7fd5cb4ddc-glg62
ff3ff45717f33       97e04611ad434                                                                                                           3 days ago          Exited              coredns                            0                   8d66621255359       coredns-5dd5756b68-2hhgn
320fd4aad9a0a       a5dd5cdd6d3ef                                                                                                           3 days ago          Exited              kube-proxy                         0                   1c3bae8408f76       kube-proxy-ldvgc
ab424460edc47       9cdd6470f48c8                                                                                                           3 days ago          Exited              etcd                               0                   32e13d34d4c7a       etcd-minikube
aa947aca4bcb0       42a4e73724daa                                                                                                           3 days ago          Exited              kube-scheduler                     0                   e3e17b49b903b       kube-scheduler-minikube
398aca05e9a89       8276439b4f237                                                                                                           3 days ago          Exited              kube-controller-manager            0                   e899cd541d84f       kube-controller-manager-minikube
58102dca432ec       537e9a59ee2fd                                                                                                           3 days ago          Exited              kube-apiserver                     0                   49387cf95f689       kube-apiserver-minikube

* 
* ==> coredns [c2765329113c] <==
* [INFO] 10.244.0.19:59505 - 40723 "A IN github.com.argocd.svc.cluster.local. udp 53 false 512" NXDOMAIN qr,aa,rd 146 0.000337541s
[INFO] 10.244.0.19:58804 - 32681 "A IN github.com.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000073167s
[INFO] 10.244.0.19:58804 - 55727 "AAAA IN github.com.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.0002705s
[INFO] 10.244.0.19:38471 - 21640 "AAAA IN github.com.cluster.local. udp 42 false 512" NXDOMAIN qr,aa,rd 135 0.000043958s
[INFO] 10.244.0.19:38471 - 11401 "A IN github.com.cluster.local. udp 42 false 512" NXDOMAIN qr,aa,rd 135 0.000066125s
[INFO] 10.244.0.19:52640 - 49355 "AAAA IN github.com. udp 28 false 512" NOERROR qr,aa,rd,ra 125 0.000034583s
[INFO] 10.244.0.19:52640 - 20170 "A IN github.com. udp 28 false 512" NOERROR qr,aa,rd,ra 54 0.000030083s
[INFO] 10.244.0.13:34074 - 15965 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000662416s
[INFO] 10.244.0.13:38915 - 54140 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000782042s
[INFO] 10.244.0.19:47995 - 34327 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.0001655s
[INFO] 10.244.0.19:58488 - 28378 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.00021175s
[INFO] 10.244.0.19:35577 - 3421 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000119333s
[INFO] 10.244.0.19:33953 - 30593 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000186209s
[INFO] 10.244.0.19:38096 - 41335 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000060834s
[INFO] 10.244.0.19:41664 - 64077 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.00011325s
[INFO] 10.244.0.19:55540 - 11944 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 125 0.005880167s
[INFO] 10.244.0.19:41737 - 2110 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.04043275s
[INFO] 10.244.0.19:51997 - 24878 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000179541s
[INFO] 10.244.0.19:47225 - 58188 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000258583s
[INFO] 10.244.0.19:37774 - 27914 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000079s
[INFO] 10.244.0.19:59705 - 41773 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000115667s
[INFO] 10.244.0.19:41938 - 17178 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000088333s
[INFO] 10.244.0.19:60050 - 10446 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.00009325s
[INFO] 10.244.0.19:45937 - 41540 "A IN github.com. udp 39 false 1232" NOERROR qr,aa,rd,ra 54 0.000057708s
[INFO] 10.244.0.19:35646 - 21630 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,aa,rd,ra 125 0.0000875s
[INFO] 10.244.0.19:58791 - 301 "AAAA IN github.com.argocd.svc.cluster.local. udp 53 false 512" NXDOMAIN qr,aa,rd 146 0.000091167s
[INFO] 10.244.0.19:40261 - 36331 "A IN github.com.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000060333s
[INFO] 10.244.0.19:58791 - 41778 "A IN github.com.argocd.svc.cluster.local. udp 53 false 512" NXDOMAIN qr,aa,rd 146 0.000198542s
[INFO] 10.244.0.19:40261 - 48149 "AAAA IN github.com.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000129875s
[INFO] 10.244.0.19:60988 - 49538 "AAAA IN github.com.cluster.local. udp 42 false 512" NXDOMAIN qr,aa,rd 135 0.000070792s
[INFO] 10.244.0.19:60988 - 40835 "A IN github.com.cluster.local. udp 42 false 512" NXDOMAIN qr,aa,rd 135 0.000337667s
[INFO] 10.244.0.19:43365 - 15594 "AAAA IN github.com. udp 28 false 512" NOERROR qr,aa,rd,ra 125 0.000038542s
[INFO] 10.244.0.19:43365 - 45548 "A IN github.com. udp 28 false 512" NOERROR qr,aa,rd,ra 54 0.000063542s
[INFO] 10.244.0.38:45372 - 58217 "A IN metadata.google.internal. udp 42 false 512" NXDOMAIN qr,rd,ra 42 0.066986125s
[INFO] 10.244.0.13:54962 - 24326 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000953458s
[INFO] 10.244.0.13:48934 - 4739 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001090208s
[INFO] 10.244.0.19:42638 - 9324 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000189167s
[INFO] 10.244.0.19:39224 - 11676 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000151708s
[INFO] 10.244.0.19:49925 - 6578 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000117334s
[INFO] 10.244.0.19:48294 - 52550 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000056208s
[INFO] 10.244.0.19:60068 - 42431 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000046791s
[INFO] 10.244.0.19:43584 - 45631 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000131333s
[INFO] 10.244.0.19:36398 - 23779 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 125 0.021506959s
[INFO] 10.244.0.19:53798 - 43160 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.073023s
[INFO] 10.244.0.19:38541 - 185 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000089792s
[INFO] 10.244.0.19:40110 - 12999 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000124167s
[INFO] 10.244.0.19:58935 - 46902 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000121916s
[INFO] 10.244.0.19:47741 - 29613 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.00004275s
[INFO] 10.244.0.19:43380 - 7987 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000044959s
[INFO] 10.244.0.19:35179 - 52694 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000049417s
[INFO] 10.244.0.19:41395 - 31250 "A IN github.com. udp 39 false 1232" NOERROR qr,aa,rd,ra 54 0.000061834s
[INFO] 10.244.0.19:38852 - 37536 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,aa,rd,ra 125 0.000075333s
[INFO] 10.244.0.19:50743 - 32049 "AAAA IN github.com.argocd.svc.cluster.local. udp 53 false 512" NXDOMAIN qr,aa,rd 146 0.000128125s
[INFO] 10.244.0.19:50743 - 1590 "A IN github.com.argocd.svc.cluster.local. udp 53 false 512" NXDOMAIN qr,aa,rd 146 0.000195125s
[INFO] 10.244.0.19:39867 - 15217 "AAAA IN github.com.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000069584s
[INFO] 10.244.0.19:39867 - 18807 "A IN github.com.svc.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000080708s
[INFO] 10.244.0.19:58763 - 18092 "AAAA IN github.com.cluster.local. udp 42 false 512" NXDOMAIN qr,aa,rd 135 0.000044708s
[INFO] 10.244.0.19:58763 - 36782 "A IN github.com.cluster.local. udp 42 false 512" NXDOMAIN qr,aa,rd 135 0.000075708s
[INFO] 10.244.0.19:33948 - 5359 "AAAA IN github.com. udp 28 false 512" NOERROR qr,aa,rd,ra 125 0.000058792s
[INFO] 10.244.0.19:33948 - 26000 "A IN github.com. udp 28 false 512" NOERROR qr,aa,rd,ra 54 0.000067334s

* 
* ==> coredns [ff3ff45717f3] <==
* [INFO] 10.244.0.8:59113 - 47043 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000093666s
[INFO] 10.244.0.8:60413 - 46192 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000051042s
[INFO] 10.244.0.8:37835 - 16282 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000066s
[INFO] 10.244.0.8:33645 - 4313 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000094791s
[INFO] 10.244.0.8:45814 - 6450 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.067938958s
[INFO] 10.244.0.8:43885 - 68 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 2.500654751s
[INFO] 10.244.0.10:47610 - 22888 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001359583s
[INFO] 10.244.0.10:40912 - 64968 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000265708s
[INFO] 10.244.0.10:58365 - 46111 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000899583s
[INFO] 10.244.0.10:59715 - 34491 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.001191167s
[INFO] 10.244.0.8:40174 - 59964 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.0001775s
[INFO] 10.244.0.8:42817 - 39012 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000038334s
[INFO] 10.244.0.8:60940 - 7022 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000118042s
[INFO] 10.244.0.8:39738 - 26447 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000035583s
[INFO] 10.244.0.8:34348 - 6217 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000045666s
[INFO] 10.244.0.8:54079 - 24159 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000036167s
[INFO] 10.244.0.8:51474 - 37526 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.020783292s
[INFO] 10.244.0.8:37006 - 39564 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.150354917s
[INFO] 10.244.0.10:36840 - 24825 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000802417s
[INFO] 10.244.0.10:59197 - 22850 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000878791s
[INFO] 10.244.0.10:52676 - 3979 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.001055667s
[INFO] 10.244.0.10:58013 - 25293 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.0014235s
[INFO] 10.244.0.8:60284 - 15003 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.00039075s
[INFO] 10.244.0.8:41481 - 1083 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000067959s
[INFO] 10.244.0.8:55959 - 59757 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000319583s
[INFO] 10.244.0.8:42668 - 9271 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000409917s
[INFO] 10.244.0.8:43844 - 6049 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000099333s
[INFO] 10.244.0.8:37409 - 7312 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000287875s
[INFO] 10.244.0.8:44922 - 52880 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.027875458s
[INFO] 10.244.0.8:53655 - 62566 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.066758542s
[INFO] 10.244.0.10:60081 - 16001 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000730875s
[INFO] 10.244.0.10:37616 - 35426 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000858625s
[INFO] 10.244.0.10:45484 - 3373 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000313625s
[INFO] 10.244.0.10:56139 - 39137 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.009249709s
[INFO] 10.244.0.8:60553 - 31222 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.00015275s
[INFO] 10.244.0.8:44989 - 11686 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000202542s
[INFO] 10.244.0.8:35834 - 56623 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.0001225s
[INFO] 10.244.0.8:34077 - 48147 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000138375s
[INFO] 10.244.0.8:57444 - 38841 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.0000775s
[INFO] 10.244.0.8:36995 - 65264 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000139s
[INFO] 10.244.0.8:58956 - 44299 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.013650667s
[INFO] 10.244.0.8:41432 - 8483 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.05858825s
[INFO] 10.244.0.10:55267 - 11160 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.003784334s
[INFO] 10.244.0.10:44296 - 37367 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000940875s
[INFO] 10.244.0.9:60097 - 29436 "AAAA IN argocd-redis.argocd.svc.cluster.local. udp 66 false 1232" NOERROR qr,aa,rd 148 0.000877833s
[INFO] 10.244.0.9:38124 - 17149 "A IN argocd-redis.argocd.svc.cluster.local. udp 66 false 1232" NOERROR qr,aa,rd 108 0.001029458s
[INFO] 10.244.0.10:39305 - 49205 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000358708s
[INFO] 10.244.0.10:44943 - 53299 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000210291s
[INFO] 10.244.0.8:34597 - 56448 "A IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000244791s
[INFO] 10.244.0.8:40662 - 4379 "AAAA IN github.com.argocd.svc.cluster.local. udp 64 false 1232" NXDOMAIN qr,aa,rd 146 0.000077375s
[INFO] 10.244.0.8:51715 - 46623 "AAAA IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000151083s
[INFO] 10.244.0.8:54211 - 24174 "A IN github.com.svc.cluster.local. udp 57 false 1232" NXDOMAIN qr,aa,rd 139 0.000347084s
[INFO] 10.244.0.8:59785 - 5693 "A IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000135875s
[INFO] 10.244.0.8:34385 - 29811 "AAAA IN github.com.cluster.local. udp 53 false 1232" NXDOMAIN qr,aa,rd 135 0.000074917s
[INFO] 10.244.0.8:48358 - 23645 "AAAA IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 112 0.0228335s
[INFO] 10.244.0.8:36447 - 58728 "A IN github.com. udp 39 false 1232" NOERROR qr,rd,ra 54 0.0479095s
[INFO] 10.244.0.10:49881 - 10205 "A IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 120 0.000889667s
[INFO] 10.244.0.10:46929 - 55421 "AAAA IN argocd-repo-server.argocd.svc.cluster.local. udp 72 false 1232" NOERROR qr,aa,rd 154 0.000981417s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_11_26T20_15_45_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 26 Nov 2023 19:15:43 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 30 Nov 2023 05:37:59 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 30 Nov 2023 05:34:07 +0000   Sun, 26 Nov 2023 19:15:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 30 Nov 2023 05:34:07 +0000   Sun, 26 Nov 2023 19:15:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 30 Nov 2023 05:34:07 +0000   Sun, 26 Nov 2023 19:15:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 30 Nov 2023 05:34:07 +0000   Sun, 26 Nov 2023 19:15:43 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                11
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8039600Ki
  pods:               110
Allocatable:
  cpu:                11
  ephemeral-storage:  61202244Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8039600Ki
  pods:               110
System Info:
  Machine ID:                 c31e1d783266490d8edbee7ccd30adcd
  System UUID:                c31e1d783266490d8edbee7ccd30adcd
  Boot ID:                    7160d6a6-8cc0-4c60-ad00-697d59f988d5
  Kernel Version:             6.4.16-linuxkit
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (18 in total)
  Namespace                   Name                                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                 ------------  ----------  ---------------  -------------  ---
  argocd                      argocd-application-controller-0                      0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  argocd                      argocd-applicationset-controller-5bf97c679b-bs9wm    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  argocd                      argocd-dex-server-f7648d898-859z5                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  argocd                      argocd-notifications-controller-6cf7579685-d8lg7     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  argocd                      argocd-redis-6976fc7dfc-jdwxc                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  argocd                      argocd-repo-server-8477fdffc7-28bll                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  argocd                      argocd-server-7c7d77f474-lls92                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  default                     esquire-microservice-77565d7689-m6qbb                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         13m
  kube-system                 coredns-5dd5756b68-2hhgn                             100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     3d10h
  kube-system                 etcd-minikube                                        100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         3d10h
  kube-system                 kube-apiserver-minikube                              250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  kube-system                 kube-controller-manager-minikube                     200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  kube-system                 kube-proxy-ldvgc                                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  kube-system                 kube-scheduler-minikube                              100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  kube-system                 metrics-server-7c66d45ddc-nhs9l                      100m (0%!)(MISSING)     0 (0%!)(MISSING)      200Mi (2%!)(MISSING)       0 (0%!)(MISSING)         31h
  kube-system                 storage-provisioner                                  0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-glg62           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-hbhzk                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d10h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (7%!)(MISSING)   0 (0%!)(MISSING)
  memory             370Mi (4%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-32Mi     0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-64Ki     0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Nov27 03:27] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.186253] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +1.317168] netlink: 'rc.init': attribute type 22 has an invalid length.
[  +0.000974] 3[335]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.260920] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.000325] FAT-fs (loop0): utf8 is not a recommended IO charset for FAT filesystems, filesystem will be case sensitive!
[  +0.015441] grpcfuse: loading out-of-tree module taints kernel.
[Nov27 08:37] hrtimer: interrupt took 3882500 ns
[Nov29 07:48] cfs_period_timer[cpu2]: period too short, scaling up (new cfs_period_us = 200000, cfs_quota_us = 400000)
[Nov29 07:51] cfs_period_timer[cpu2]: period too short, scaling up (new cfs_period_us = 400000, cfs_quota_us = 800000)

* 
* ==> etcd [9b5e2153e7fc] <==
* {"level":"info","ts":"2023-11-30T04:02:57.07358Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":183810,"took":"9.703792ms","hash":27690414}
{"level":"info","ts":"2023-11-30T04:02:57.073634Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":27690414,"revision":183810,"compact-revision":183567}
{"level":"info","ts":"2023-11-30T04:07:57.077964Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":184050}
{"level":"info","ts":"2023-11-30T04:07:57.08521Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":184050,"took":"6.595125ms","hash":3381709084}
{"level":"info","ts":"2023-11-30T04:07:57.08527Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3381709084,"revision":184050,"compact-revision":183810}
{"level":"info","ts":"2023-11-30T04:12:57.081832Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":184292}
{"level":"info","ts":"2023-11-30T04:12:57.087762Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":184292,"took":"5.349208ms","hash":297032388}
{"level":"info","ts":"2023-11-30T04:12:57.087802Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":297032388,"revision":184292,"compact-revision":184050}
{"level":"info","ts":"2023-11-30T04:17:57.10019Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":184532}
{"level":"info","ts":"2023-11-30T04:17:57.109353Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":184532,"took":"7.957208ms","hash":1963346869}
{"level":"info","ts":"2023-11-30T04:17:57.10941Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1963346869,"revision":184532,"compact-revision":184292}
{"level":"info","ts":"2023-11-30T04:22:57.106619Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":184773}
{"level":"info","ts":"2023-11-30T04:22:57.111892Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":184773,"took":"4.763042ms","hash":3982223626}
{"level":"info","ts":"2023-11-30T04:22:57.111935Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3982223626,"revision":184773,"compact-revision":184532}
{"level":"info","ts":"2023-11-30T04:27:57.114262Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":185016}
{"level":"info","ts":"2023-11-30T04:27:57.1181Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":185016,"took":"3.470833ms","hash":2391207560}
{"level":"info","ts":"2023-11-30T04:27:57.118118Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2391207560,"revision":185016,"compact-revision":184773}
{"level":"info","ts":"2023-11-30T04:32:57.139559Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":185255}
{"level":"info","ts":"2023-11-30T04:32:57.144525Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":185255,"took":"4.042875ms","hash":2291740519}
{"level":"info","ts":"2023-11-30T04:32:57.144557Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2291740519,"revision":185255,"compact-revision":185016}
{"level":"info","ts":"2023-11-30T04:37:57.164999Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":185496}
{"level":"info","ts":"2023-11-30T04:37:57.1731Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":185496,"took":"6.705875ms","hash":2856598279}
{"level":"info","ts":"2023-11-30T04:37:57.173131Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2856598279,"revision":185496,"compact-revision":185255}
{"level":"info","ts":"2023-11-30T04:42:57.171295Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":185738}
{"level":"info","ts":"2023-11-30T04:42:57.179215Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":185738,"took":"6.515875ms","hash":611946324}
{"level":"info","ts":"2023-11-30T04:42:57.179261Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":611946324,"revision":185738,"compact-revision":185496}
{"level":"info","ts":"2023-11-30T04:47:57.191752Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":185977}
{"level":"info","ts":"2023-11-30T04:47:57.209254Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":185977,"took":"12.201125ms","hash":2099186561}
{"level":"info","ts":"2023-11-30T04:47:57.209329Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2099186561,"revision":185977,"compact-revision":185738}
{"level":"info","ts":"2023-11-30T04:48:50.263382Z","caller":"wal/wal.go:785","msg":"created a new WAL segment","path":"/var/lib/minikube/etcd/member/wal/0000000000000003-0000000000038e54.wal"}
{"level":"info","ts":"2023-11-30T04:52:57.213905Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":186219}
{"level":"info","ts":"2023-11-30T04:52:57.229171Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":186219,"took":"14.401167ms","hash":710162154}
{"level":"info","ts":"2023-11-30T04:52:57.229228Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":710162154,"revision":186219,"compact-revision":185977}
{"level":"info","ts":"2023-11-30T04:57:57.223145Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":186461}
{"level":"info","ts":"2023-11-30T04:57:57.22731Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":186461,"took":"3.571292ms","hash":1560494221}
{"level":"info","ts":"2023-11-30T04:57:57.227333Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1560494221,"revision":186461,"compact-revision":186219}
{"level":"info","ts":"2023-11-30T05:02:57.229252Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":186741}
{"level":"info","ts":"2023-11-30T05:02:57.235079Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":186741,"took":"4.740375ms","hash":3591948340}
{"level":"info","ts":"2023-11-30T05:02:57.235098Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3591948340,"revision":186741,"compact-revision":186461}
{"level":"info","ts":"2023-11-30T05:07:57.235072Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":186980}
{"level":"info","ts":"2023-11-30T05:07:57.239793Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":186980,"took":"4.086416ms","hash":4017865269}
{"level":"info","ts":"2023-11-30T05:07:57.239844Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4017865269,"revision":186980,"compact-revision":186741}
{"level":"info","ts":"2023-11-30T05:12:57.262897Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":187219}
{"level":"info","ts":"2023-11-30T05:12:57.27697Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":187219,"took":"12.737584ms","hash":3594554628}
{"level":"info","ts":"2023-11-30T05:12:57.277018Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3594554628,"revision":187219,"compact-revision":186980}
{"level":"info","ts":"2023-11-30T05:17:57.259122Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":187459}
{"level":"info","ts":"2023-11-30T05:17:57.269583Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":187459,"took":"9.6785ms","hash":1002023057}
{"level":"info","ts":"2023-11-30T05:17:57.269652Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1002023057,"revision":187459,"compact-revision":187219}
{"level":"info","ts":"2023-11-30T05:22:57.278611Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":187698}
{"level":"info","ts":"2023-11-30T05:22:57.287573Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":187698,"took":"7.558208ms","hash":2043254546}
{"level":"info","ts":"2023-11-30T05:22:57.287609Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2043254546,"revision":187698,"compact-revision":187459}
{"level":"info","ts":"2023-11-30T05:27:57.293381Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":187937}
{"level":"info","ts":"2023-11-30T05:27:57.304562Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":187937,"took":"9.391291ms","hash":4046783556}
{"level":"info","ts":"2023-11-30T05:27:57.30462Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4046783556,"revision":187937,"compact-revision":187698}
{"level":"info","ts":"2023-11-30T05:32:57.330797Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":188201}
{"level":"info","ts":"2023-11-30T05:32:57.337653Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":188201,"took":"5.36175ms","hash":1516651632}
{"level":"info","ts":"2023-11-30T05:32:57.337729Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1516651632,"revision":188201,"compact-revision":187937}
{"level":"info","ts":"2023-11-30T05:37:57.337806Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":188440}
{"level":"info","ts":"2023-11-30T05:37:57.346683Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":188440,"took":"7.755375ms","hash":642806697}
{"level":"info","ts":"2023-11-30T05:37:57.346757Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":642806697,"revision":188440,"compact-revision":188201}

* 
* ==> etcd [ab424460edc4] <==
* {"level":"info","ts":"2023-11-28T13:22:15.49009Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":102649,"took":"2.469542ms","hash":2440208849}
{"level":"info","ts":"2023-11-28T13:22:15.490112Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2440208849,"revision":102649,"compact-revision":102409}
{"level":"info","ts":"2023-11-28T13:27:15.484675Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":102891}
{"level":"info","ts":"2023-11-28T13:27:15.489062Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":102891,"took":"3.781084ms","hash":3251300330}
{"level":"info","ts":"2023-11-28T13:27:15.489117Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3251300330,"revision":102891,"compact-revision":102649}
{"level":"info","ts":"2023-11-28T13:32:15.486243Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":103132}
{"level":"info","ts":"2023-11-28T13:32:15.493568Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":103132,"took":"6.629667ms","hash":945645720}
{"level":"info","ts":"2023-11-28T13:32:15.493639Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":945645720,"revision":103132,"compact-revision":102891}
{"level":"info","ts":"2023-11-28T13:37:15.482224Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":103373}
{"level":"info","ts":"2023-11-28T13:37:15.496654Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":103373,"took":"10.812584ms","hash":3219947096}
{"level":"info","ts":"2023-11-28T13:37:15.496696Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3219947096,"revision":103373,"compact-revision":103132}
{"level":"info","ts":"2023-11-28T13:42:15.488929Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":103616}
{"level":"info","ts":"2023-11-28T13:42:15.494524Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":103616,"took":"4.646417ms","hash":540447719}
{"level":"info","ts":"2023-11-28T13:42:15.494586Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":540447719,"revision":103616,"compact-revision":103373}
{"level":"info","ts":"2023-11-28T13:42:49.597641Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":130013,"local-member-snapshot-index":120012,"local-member-snapshot-count":10000}
{"level":"info","ts":"2023-11-28T13:42:49.604084Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":130013}
{"level":"info","ts":"2023-11-28T13:42:49.604746Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":125013}
{"level":"info","ts":"2023-11-28T13:42:50.68174Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/minikube/etcd/member/snap/0000000000000002-0000000000013888.snap"}
{"level":"info","ts":"2023-11-28T13:47:15.497026Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":103857}
{"level":"info","ts":"2023-11-28T13:47:15.502933Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":103857,"took":"5.167583ms","hash":2444694149}
{"level":"info","ts":"2023-11-28T13:47:15.503Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2444694149,"revision":103857,"compact-revision":103616}
{"level":"info","ts":"2023-11-28T13:52:15.491445Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":104099}
{"level":"info","ts":"2023-11-28T13:52:15.49567Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":104099,"took":"3.533916ms","hash":4046771458}
{"level":"info","ts":"2023-11-28T13:52:15.495731Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4046771458,"revision":104099,"compact-revision":103857}
{"level":"info","ts":"2023-11-28T13:57:19.947464Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":104341}
{"level":"info","ts":"2023-11-28T13:57:19.960953Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":104341,"took":"2.679458ms","hash":3229994211}
{"level":"info","ts":"2023-11-28T13:57:19.961002Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3229994211,"revision":104341,"compact-revision":104099}
{"level":"info","ts":"2023-11-28T14:02:19.949253Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":104582}
{"level":"info","ts":"2023-11-28T14:02:19.952961Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":104582,"took":"3.113417ms","hash":2806322784}
{"level":"info","ts":"2023-11-28T14:02:19.953002Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2806322784,"revision":104582,"compact-revision":104341}
{"level":"info","ts":"2023-11-28T14:07:19.959468Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":104823}
{"level":"info","ts":"2023-11-28T14:07:19.96323Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":104823,"took":"2.915166ms","hash":198957762}
{"level":"info","ts":"2023-11-28T14:07:19.963262Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":198957762,"revision":104823,"compact-revision":104582}
{"level":"info","ts":"2023-11-28T14:09:33.723855Z","caller":"traceutil/trace.go:171","msg":"trace[29665330] transaction","detail":"{read_only:false; response_revision:105170; number_of_response:1; }","duration":"205.621292ms","start":"2023-11-28T14:09:33.517201Z","end":"2023-11-28T14:09:33.722823Z","steps":["trace[29665330] 'process raft request'  (duration: 201.682542ms)"],"step_count":1}
{"level":"info","ts":"2023-11-28T14:09:33.724056Z","caller":"traceutil/trace.go:171","msg":"trace[984647032] linearizableReadLoop","detail":"{readStateIndex:131623; appliedIndex:131622; }","duration":"104.892291ms","start":"2023-11-28T14:09:33.617121Z","end":"2023-11-28T14:09:33.722014Z","steps":["trace[984647032] 'read index received'  (duration: 101.309083ms)","trace[984647032] 'applied index is now lower than readState.Index'  (duration: 3.582375ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-28T14:09:34.020439Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"401.099083ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-11-28T14:09:34.021399Z","caller":"traceutil/trace.go:171","msg":"trace[639327471] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:105170; }","duration":"403.60825ms","start":"2023-11-28T14:09:33.617072Z","end":"2023-11-28T14:09:34.02068Z","steps":["trace[639327471] 'agreement among raft nodes before linearized reading'  (duration: 107.895667ms)","trace[639327471] 'range keys from in-memory index tree'  (duration: 292.840125ms)"],"step_count":2}
{"level":"warn","ts":"2023-11-28T14:09:34.021506Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-11-28T14:09:33.617058Z","time spent":"404.379625ms","remote":"127.0.0.1:46650","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2023-11-28T14:12:19.83242Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":105063}
{"level":"info","ts":"2023-11-28T14:12:19.840865Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":105063,"took":"7.233625ms","hash":105230544}
{"level":"info","ts":"2023-11-28T14:12:19.841062Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":105230544,"revision":105063,"compact-revision":104823}
{"level":"info","ts":"2023-11-28T14:17:19.828468Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":105304}
{"level":"info","ts":"2023-11-28T14:17:19.840859Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":105304,"took":"11.288625ms","hash":1809853496}
{"level":"info","ts":"2023-11-28T14:17:19.84092Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1809853496,"revision":105304,"compact-revision":105063}
{"level":"info","ts":"2023-11-28T14:22:19.822438Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":105545}
{"level":"info","ts":"2023-11-28T14:22:19.829766Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":105545,"took":"4.758958ms","hash":553621705}
{"level":"info","ts":"2023-11-28T14:22:19.829873Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":553621705,"revision":105545,"compact-revision":105304}
{"level":"info","ts":"2023-11-28T14:27:20.039227Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":105786}
{"level":"info","ts":"2023-11-28T14:27:20.045156Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":105786,"took":"4.576292ms","hash":1468516109}
{"level":"info","ts":"2023-11-28T14:27:20.045195Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1468516109,"revision":105786,"compact-revision":105545}
{"level":"info","ts":"2023-11-28T14:27:34.987985Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-11-28T14:27:34.988865Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2023-11-28T14:27:34.99014Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2023-11-28T14:27:34.990653Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2023-11-28T14:27:34.990988Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2023-11-28T14:27:34.991209Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"info","ts":"2023-11-28T14:27:35.106327Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2023-11-28T14:27:35.187145Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-11-28T14:27:35.187466Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2023-11-28T14:27:35.187478Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> kernel <==
*  05:38:00 up 3 days,  2:10,  0 users,  load average: 0.77, 0.66, 0.75
Linux minikube 6.4.16-linuxkit #1 SMP PREEMPT Sat Sep 23 13:36:48 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [5203fc224b16] <==
* I1130 05:03:02.771442       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:03:54.563024       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:04:54.571683       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:05:54.552252       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:06:54.542326       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:07:54.563786       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:07:55.571841       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:07:55.572025       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:07:55.572114       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:08:02.794924       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:08:54.545482       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:09:54.557073       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:10:54.565523       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:11:54.560439       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:12:54.584846       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:12:55.595808       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:12:55.596923       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:12:55.597082       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:13:02.817458       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:13:54.563390       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:14:54.546750       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:15:54.544828       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:16:54.563216       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:17:54.540265       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:17:55.565615       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:17:55.565725       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:18:02.812041       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:18:54.562938       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:19:54.569880       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:20:54.549956       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:21:54.579666       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:22:54.550143       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:22:55.584648       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:22:55.589872       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:23:02.832737       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:23:54.569430       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:24:54.562776       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:24:55.128118       1 alloc.go:330] "allocated clusterIPs" service="default/esquire-service" clusterIPs={"IPv4":"10.97.114.241"}
I1130 05:25:54.621513       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:26:54.568827       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:27:54.564470       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:27:55.582252       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:27:55.582392       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:28:02.847452       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:28:54.565008       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:29:54.571083       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:30:54.581669       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:31:54.552679       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:32:54.555929       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:32:55.584319       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:32:55.585279       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:32:55.586147       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:33:02.874309       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:33:54.557125       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:34:54.570499       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:35:54.572336       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:36:54.577277       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:37:54.555085       1 handler.go:232] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
I1130 05:37:55.597458       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager
I1130 05:37:55.597628       1 handler.go:232] Adding GroupVersion argoproj.io v1alpha1 to ResourceManager

* 
* ==> kube-apiserver [58102dca432e] <==
*   "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1128 14:27:36.001110       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1128 14:27:36.001380       1 logging.go:59] [core] [Channel #31 SubChannel #32] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1128 14:27:36.001489       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1128 14:27:36.001573       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1128 14:27:36.001699       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1128 14:27:36.001805       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1128 14:27:36.003186       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"

* 
* ==> kube-controller-manager [398aca05e9a8] <==
* I1126 19:36:30.988829       1 event.go:307] "Event occurred" object="argocd/argocd-server" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set argocd-server-7c7d77f474 to 1"
I1126 19:36:30.993468       1 event.go:307] "Event occurred" object="argocd/argocd-server-7c7d77f474" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: argocd-server-7c7d77f474-lls92"
I1126 19:36:30.993819       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-repo-server-8477fdffc7" duration="45.666µs"
I1126 19:36:30.998935       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-server-7c7d77f474" duration="10.285041ms"
I1126 19:36:31.004906       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-server-7c7d77f474" duration="5.92475ms"
I1126 19:36:31.004989       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-server-7c7d77f474" duration="52.625µs"
I1126 19:36:31.007779       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-server-7c7d77f474" duration="72.416µs"
I1126 19:36:31.017431       1 event.go:307] "Event occurred" object="argocd/argocd-application-controller" fieldPath="" kind="StatefulSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="create Pod argocd-application-controller-0 in StatefulSet argocd-application-controller successful"
I1126 19:36:58.223221       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-dex-server-f7648d898" duration="167.333µs"
I1126 19:36:59.233944       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-applicationset-controller-5bf97c679b" duration="4.724875ms"
I1126 19:36:59.234038       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-applicationset-controller-5bf97c679b" duration="52.417µs"
I1126 19:36:59.289382       1 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="appprojects.argoproj.io"
I1126 19:36:59.289411       1 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="applications.argoproj.io"
I1126 19:36:59.289442       1 resource_quota_monitor.go:224] "QuotaMonitor created object count evaluator" resource="applicationsets.argoproj.io"
I1126 19:36:59.289514       1 shared_informer.go:311] Waiting for caches to sync for resource quota
I1126 19:36:59.390468       1 shared_informer.go:318] Caches are synced for resource quota
I1126 19:36:59.413316       1 shared_informer.go:311] Waiting for caches to sync for garbage collector
I1126 19:36:59.413343       1 shared_informer.go:318] Caches are synced for garbage collector
I1126 19:37:07.352908       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-redis-6976fc7dfc" duration="5.446834ms"
I1126 19:37:07.353159       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-redis-6976fc7dfc" duration="182.125µs"
I1126 19:37:08.422245       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-repo-server-8477fdffc7" duration="205.959µs"
I1126 19:37:10.464626       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-server-7c7d77f474" duration="154.208µs"
I1126 19:37:13.521489       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-notifications-controller-6cf7579685" duration="5.289542ms"
I1126 19:37:13.521615       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-notifications-controller-6cf7579685" duration="68.666µs"
I1126 19:37:20.612048       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-dex-server-f7648d898" duration="5.555167ms"
I1126 19:37:20.612137       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-dex-server-f7648d898" duration="58.875µs"
I1126 19:37:22.639246       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-repo-server-8477fdffc7" duration="154.416µs"
I1126 19:37:31.312547       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-repo-server-8477fdffc7" duration="10.578375ms"
I1126 19:37:31.312665       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-repo-server-8477fdffc7" duration="73.041µs"
I1126 19:37:31.324818       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-server-7c7d77f474" duration="6.900209ms"
I1126 19:37:31.324920       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-server-7c7d77f474" duration="74.334µs"
I1126 20:22:55.935992       1 event.go:307] "Event occurred" object="default/guestbook-ui" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set guestbook-ui-56c646849b to 1"
I1126 20:22:55.946123       1 event.go:307] "Event occurred" object="default/guestbook-ui-56c646849b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: guestbook-ui-56c646849b-md45g"
I1126 20:22:55.949603       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/guestbook-ui-56c646849b" duration="13.860917ms"
I1126 20:22:55.953864       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/guestbook-ui-56c646849b" duration="4.201709ms"
I1126 20:22:55.953944       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/guestbook-ui-56c646849b" duration="26µs"
I1126 20:22:55.958528       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/guestbook-ui-56c646849b" duration="34.5µs"
I1126 20:23:09.398578       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/guestbook-ui-56c646849b" duration="7.457625ms"
I1126 20:23:09.398782       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/guestbook-ui-56c646849b" duration="89.292µs"
I1126 21:15:45.014822       1 cleaner.go:175] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="csr-z9whd" approvedExpiration="1h0m0s"
E1127 06:31:56.584821       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1127 06:31:56.584933       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
I1127 08:34:02.889272       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1127 08:34:02.891530       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1127 18:48:57.768110       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1127 18:48:57.768483       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
E1127 20:17:54.137006       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1127 20:17:54.137047       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
I1127 22:52:07.749804       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-repo-server-8477fdffc7" duration="2.417625ms"
I1127 22:52:07.749846       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="6.260167ms"
I1127 22:52:07.750202       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="78.291µs"
I1127 22:52:07.750256       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="249.042µs"
I1127 22:52:07.750407       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-notifications-controller-6cf7579685" duration="168.458µs"
I1127 22:52:07.750455       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-redis-6976fc7dfc" duration="152.834µs"
I1127 22:52:07.749836       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-dex-server-f7648d898" duration="2.513666ms"
I1127 22:52:07.749837       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-server-7c7d77f474" duration="1.884708ms"
I1127 22:52:07.749842       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-applicationset-controller-5bf97c679b" duration="4.712916ms"
I1127 22:52:07.750620       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/guestbook-ui-56c646849b" duration="177.584µs"
E1128 08:53:59.314255       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1128 08:53:59.314017       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"

* 
* ==> kube-controller-manager [ad74500e50d3] <==
* I1129 05:24:13.477512       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="9.418667ms"
I1129 05:24:13.478042       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="370.75µs"
I1129 05:24:13.478303       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="84.042µs"
I1129 05:24:13.510723       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="75.792µs"
I1129 05:24:15.514308       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="5.834334ms"
I1129 05:24:15.514403       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="56.416µs"
I1129 05:26:02.616335       1 event.go:307] "Event occurred" object="default/esquire-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set esquire-deployment-7755f7878f to 2 from 3"
I1129 05:26:02.647671       1 event.go:307] "Event occurred" object="default/esquire-deployment-7755f7878f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: esquire-deployment-7755f7878f-s5zz9"
I1129 05:26:02.690665       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="75.023458ms"
I1129 05:26:02.702442       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="11.50275ms"
I1129 05:26:02.702507       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="34.708µs"
I1129 05:26:03.952468       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="95.791µs"
I1129 05:26:04.474054       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="36.25µs"
I1129 05:26:04.481237       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="257.25µs"
I1129 05:26:04.490302       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="232.834µs"
I1129 07:53:49.673416       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1129 07:53:49.765003       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1129 08:57:18.094277       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1129 08:57:18.114031       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1129 16:49:17.841585       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-server-7c7d77f474" duration="3.249334ms"
I1129 16:49:17.842231       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="67.042µs"
I1129 16:49:17.841584       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="3.2725ms"
I1129 16:49:17.842412       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7ff8df7b8b" duration="152.917µs"
I1129 16:49:17.841586       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-applicationset-controller-5bf97c679b" duration="3.565167ms"
I1129 16:49:17.842487       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-redis-6976fc7dfc" duration="42.542µs"
I1129 16:49:17.842502       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-notifications-controller-6cf7579685" duration="118.625µs"
I1129 16:49:17.842582       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/metrics-server-7c66d45ddc" duration="82.875µs"
I1129 16:49:17.842586       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="72.084µs"
I1129 16:49:17.841585       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-repo-server-8477fdffc7" duration="3.127166ms"
I1129 16:49:17.841592       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="argocd/argocd-dex-server-f7648d898" duration="3.161417ms"
I1129 16:49:17.842664       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="141.292µs"
I1129 21:16:26.263313       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1129 21:16:26.263122       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1129 22:20:54.794381       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
E1129 22:20:54.794447       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
E1130 00:21:17.655972       1 resource_quota_controller.go:440] failed to discover resources: the server has asked for the client to provide credentials
I1130 00:21:17.656058       1 garbagecollector.go:818] "failed to discover preferred resources" error="the server has asked for the client to provide credentials"
I1130 04:56:22.611632       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7ff8df7b8b" duration="301.25µs"
I1130 04:56:22.632225       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="19.267042ms"
I1130 04:56:22.645697       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="13.390791ms"
I1130 04:56:22.655738       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="9.986583ms"
I1130 04:56:22.656523       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="743µs"
I1130 04:56:23.695356       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="393.333µs"
I1130 04:56:23.703150       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="62.292µs"
I1130 04:56:24.032456       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="86.417µs"
I1130 04:56:24.038796       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="240.5µs"
I1130 04:56:24.040336       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="43.708µs"
I1130 04:56:24.045748       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="39.541µs"
I1130 04:56:24.055777       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="49.084µs"
I1130 04:56:24.058580       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="31.625µs"
I1130 04:56:24.067827       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-deployment-7755f7878f" duration="7.625µs"
I1130 05:24:55.154100       1 event.go:307] "Event occurred" object="default/esquire-microservice" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set esquire-microservice-77565d7689 to 1"
I1130 05:24:55.163745       1 event.go:307] "Event occurred" object="default/esquire-microservice-77565d7689" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: esquire-microservice-77565d7689-m6qbb"
I1130 05:24:55.168311       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-microservice-77565d7689" duration="15.062292ms"
I1130 05:24:55.172394       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-microservice-77565d7689" duration="4.028333ms"
I1130 05:24:55.172628       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-microservice-77565d7689" duration="85.667µs"
I1130 05:24:55.178853       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-microservice-77565d7689" duration="44.791µs"
I1130 05:24:55.191229       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-microservice-77565d7689" duration="54.834µs"
I1130 05:24:56.135379       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-microservice-77565d7689" duration="3.134916ms"
I1130 05:24:56.135679       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/esquire-microservice-77565d7689" duration="256.084µs"

* 
* ==> kube-proxy [320fd4aad9a0] <==
* I1126 19:15:59.860589       1 server_others.go:69] "Using iptables proxy"
I1126 19:15:59.867662       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1126 19:15:59.884694       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1126 19:15:59.885895       1 server_others.go:152] "Using iptables Proxier"
I1126 19:15:59.885923       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1126 19:15:59.885928       1 server_others.go:438] "Defaulting to no-op detect-local"
I1126 19:15:59.885982       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1126 19:15:59.886171       1 server.go:846] "Version info" version="v1.28.3"
I1126 19:15:59.886180       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1126 19:15:59.886768       1 config.go:188] "Starting service config controller"
I1126 19:15:59.886798       1 config.go:97] "Starting endpoint slice config controller"
I1126 19:15:59.886817       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1126 19:15:59.886818       1 shared_informer.go:311] Waiting for caches to sync for service config
I1126 19:15:59.886839       1 config.go:315] "Starting node config controller"
I1126 19:15:59.886861       1 shared_informer.go:311] Waiting for caches to sync for node config
I1126 19:15:59.986993       1 shared_informer.go:318] Caches are synced for service config
I1126 19:15:59.987013       1 shared_informer.go:318] Caches are synced for node config
I1126 19:15:59.986995       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-proxy [575de83ad1d0] <==
* I1128 22:04:01.524255       1 server_others.go:69] "Using iptables proxy"
I1128 22:04:01.616801       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1128 22:04:02.009977       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1128 22:04:02.016498       1 server_others.go:152] "Using iptables Proxier"
I1128 22:04:02.016538       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1128 22:04:02.016563       1 server_others.go:438] "Defaulting to no-op detect-local"
I1128 22:04:02.016956       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1128 22:04:02.017653       1 server.go:846] "Version info" version="v1.28.3"
I1128 22:04:02.017669       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1128 22:04:02.099677       1 config.go:188] "Starting service config controller"
I1128 22:04:02.100060       1 config.go:97] "Starting endpoint slice config controller"
I1128 22:04:02.100262       1 shared_informer.go:311] Waiting for caches to sync for service config
I1128 22:04:02.100269       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1128 22:04:02.100418       1 config.go:315] "Starting node config controller"
I1128 22:04:02.100427       1 shared_informer.go:311] Waiting for caches to sync for node config
I1128 22:04:02.200544       1 shared_informer.go:318] Caches are synced for node config
I1128 22:04:02.200571       1 shared_informer.go:318] Caches are synced for service config
I1128 22:04:02.203900       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [06924824e1f0] <==
* I1128 22:03:54.525075       1 serving.go:348] Generated self-signed cert in-memory
W1128 22:03:57.406145       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1128 22:03:57.406322       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1128 22:03:57.406334       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1128 22:03:57.406338       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1128 22:03:57.420555       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I1128 22:03:57.420581       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1128 22:03:57.502805       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1128 22:03:57.502853       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1128 22:03:57.502917       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1128 22:03:57.502829       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1128 22:03:57.604163       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [aa947aca4bcb] <==
* I1126 19:15:42.186098       1 serving.go:348] Generated self-signed cert in-memory
W1126 19:15:43.161186       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1126 19:15:43.161204       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1126 19:15:43.161210       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W1126 19:15:43.161213       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1126 19:15:43.167894       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I1126 19:15:43.167912       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1126 19:15:43.168979       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1126 19:15:43.169039       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1126 19:15:43.170067       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1126 19:15:43.170115       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W1126 19:15:43.170315       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1126 19:15:43.170356       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1126 19:15:43.170827       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1126 19:15:43.170844       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1126 19:15:43.171524       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1126 19:15:43.171544       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1126 19:15:43.171565       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1126 19:15:43.171570       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1126 19:15:43.171577       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1126 19:15:43.171574       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1126 19:15:43.171586       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1126 19:15:43.171619       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1126 19:15:43.171608       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1126 19:15:43.171639       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1126 19:15:43.171685       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1126 19:15:43.171696       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1126 19:15:43.171713       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1126 19:15:43.171717       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1126 19:15:43.171779       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1126 19:15:43.171794       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1126 19:15:43.171826       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1126 19:15:43.171845       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1126 19:15:43.171825       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1126 19:15:43.171857       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1126 19:15:43.171876       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1126 19:15:43.171887       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1126 19:15:43.171929       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1126 19:15:43.171961       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1126 19:15:43.171966       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1126 19:15:43.171974       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1126 19:15:43.982405       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1126 19:15:43.982428       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
I1126 19:15:44.370297       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1128 14:27:34.697944       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1128 14:27:34.698106       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I1128 14:27:34.698725       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E1128 14:27:34.699930       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Nov 30 04:12:49 minikube kubelet[1505]: W1130 04:12:49.692502    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 04:17:49 minikube kubelet[1505]: W1130 04:17:49.696635    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 04:17:49 minikube kubelet[1505]: W1130 04:17:49.700417    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 04:22:49 minikube kubelet[1505]: W1130 04:22:49.696193    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 04:22:49 minikube kubelet[1505]: W1130 04:22:49.698951    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 04:27:49 minikube kubelet[1505]: W1130 04:27:49.697315    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 04:27:49 minikube kubelet[1505]: W1130 04:27:49.699286    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 04:32:49 minikube kubelet[1505]: W1130 04:32:49.700402    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 04:32:49 minikube kubelet[1505]: W1130 04:32:49.701927    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 04:37:49 minikube kubelet[1505]: W1130 04:37:49.701272    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 04:37:49 minikube kubelet[1505]: W1130 04:37:49.704567    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 04:42:49 minikube kubelet[1505]: W1130 04:42:49.701168    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 04:42:49 minikube kubelet[1505]: W1130 04:42:49.702737    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 04:47:49 minikube kubelet[1505]: W1130 04:47:49.703694    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 04:47:49 minikube kubelet[1505]: W1130 04:47:49.705996    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 04:52:49 minikube kubelet[1505]: W1130 04:52:49.701512    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 04:52:49 minikube kubelet[1505]: W1130 04:52:49.703045    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 04:56:23 minikube kubelet[1505]: I1130 04:56:23.798323    1505 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-kr5pc\" (UniqueName: \"kubernetes.io/projected/9b6b984b-a658-4a1e-842b-1dd007ab063e-kube-api-access-kr5pc\") pod \"9b6b984b-a658-4a1e-842b-1dd007ab063e\" (UID: \"9b6b984b-a658-4a1e-842b-1dd007ab063e\") "
Nov 30 04:56:23 minikube kubelet[1505]: I1130 04:56:23.798384    1505 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-8mwt4\" (UniqueName: \"kubernetes.io/projected/794683b5-3097-44f6-b6a6-9d1247b9f670-kube-api-access-8mwt4\") pod \"794683b5-3097-44f6-b6a6-9d1247b9f670\" (UID: \"794683b5-3097-44f6-b6a6-9d1247b9f670\") "
Nov 30 04:56:23 minikube kubelet[1505]: I1130 04:56:23.804846    1505 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/9b6b984b-a658-4a1e-842b-1dd007ab063e-kube-api-access-kr5pc" (OuterVolumeSpecName: "kube-api-access-kr5pc") pod "9b6b984b-a658-4a1e-842b-1dd007ab063e" (UID: "9b6b984b-a658-4a1e-842b-1dd007ab063e"). InnerVolumeSpecName "kube-api-access-kr5pc". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 30 04:56:23 minikube kubelet[1505]: I1130 04:56:23.804845    1505 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/794683b5-3097-44f6-b6a6-9d1247b9f670-kube-api-access-8mwt4" (OuterVolumeSpecName: "kube-api-access-8mwt4") pod "794683b5-3097-44f6-b6a6-9d1247b9f670" (UID: "794683b5-3097-44f6-b6a6-9d1247b9f670"). InnerVolumeSpecName "kube-api-access-8mwt4". PluginName "kubernetes.io/projected", VolumeGidValue ""
Nov 30 04:56:23 minikube kubelet[1505]: I1130 04:56:23.898802    1505 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-kr5pc\" (UniqueName: \"kubernetes.io/projected/9b6b984b-a658-4a1e-842b-1dd007ab063e-kube-api-access-kr5pc\") on node \"minikube\" DevicePath \"\""
Nov 30 04:56:23 minikube kubelet[1505]: I1130 04:56:23.898848    1505 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-8mwt4\" (UniqueName: \"kubernetes.io/projected/794683b5-3097-44f6-b6a6-9d1247b9f670-kube-api-access-8mwt4\") on node \"minikube\" DevicePath \"\""
Nov 30 04:56:24 minikube kubelet[1505]: I1130 04:56:24.026535    1505 scope.go:117] "RemoveContainer" containerID="4b27cebc9e62336124a06ec17b34342795bc9d2bd24db0464950393c94c66a28"
Nov 30 04:56:24 minikube kubelet[1505]: I1130 04:56:24.047046    1505 scope.go:117] "RemoveContainer" containerID="4b27cebc9e62336124a06ec17b34342795bc9d2bd24db0464950393c94c66a28"
Nov 30 04:56:24 minikube kubelet[1505]: E1130 04:56:24.049246    1505 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 4b27cebc9e62336124a06ec17b34342795bc9d2bd24db0464950393c94c66a28" containerID="4b27cebc9e62336124a06ec17b34342795bc9d2bd24db0464950393c94c66a28"
Nov 30 04:56:24 minikube kubelet[1505]: I1130 04:56:24.049341    1505 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"4b27cebc9e62336124a06ec17b34342795bc9d2bd24db0464950393c94c66a28"} err="failed to get container status \"4b27cebc9e62336124a06ec17b34342795bc9d2bd24db0464950393c94c66a28\": rpc error: code = Unknown desc = Error response from daemon: No such container: 4b27cebc9e62336124a06ec17b34342795bc9d2bd24db0464950393c94c66a28"
Nov 30 04:56:24 minikube kubelet[1505]: I1130 04:56:24.049355    1505 scope.go:117] "RemoveContainer" containerID="c8b825f126e55b108e94de42e0289c75a605ee1fc607f3a5fd59bdd9b681e4df"
Nov 30 04:56:24 minikube kubelet[1505]: I1130 04:56:24.072978    1505 scope.go:117] "RemoveContainer" containerID="c8b825f126e55b108e94de42e0289c75a605ee1fc607f3a5fd59bdd9b681e4df"
Nov 30 04:56:24 minikube kubelet[1505]: E1130 04:56:24.077649    1505 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: c8b825f126e55b108e94de42e0289c75a605ee1fc607f3a5fd59bdd9b681e4df" containerID="c8b825f126e55b108e94de42e0289c75a605ee1fc607f3a5fd59bdd9b681e4df"
Nov 30 04:56:24 minikube kubelet[1505]: I1130 04:56:24.077700    1505 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"c8b825f126e55b108e94de42e0289c75a605ee1fc607f3a5fd59bdd9b681e4df"} err="failed to get container status \"c8b825f126e55b108e94de42e0289c75a605ee1fc607f3a5fd59bdd9b681e4df\": rpc error: code = Unknown desc = Error response from daemon: No such container: c8b825f126e55b108e94de42e0289c75a605ee1fc607f3a5fd59bdd9b681e4df"
Nov 30 04:56:25 minikube kubelet[1505]: I1130 04:56:25.639243    1505 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="794683b5-3097-44f6-b6a6-9d1247b9f670" path="/var/lib/kubelet/pods/794683b5-3097-44f6-b6a6-9d1247b9f670/volumes"
Nov 30 04:56:25 minikube kubelet[1505]: I1130 04:56:25.640185    1505 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="9b6b984b-a658-4a1e-842b-1dd007ab063e" path="/var/lib/kubelet/pods/9b6b984b-a658-4a1e-842b-1dd007ab063e/volumes"
Nov 30 04:57:49 minikube kubelet[1505]: W1130 04:57:49.705861    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 04:57:49 minikube kubelet[1505]: W1130 04:57:49.707781    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 05:02:49 minikube kubelet[1505]: W1130 05:02:49.716873    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 05:02:49 minikube kubelet[1505]: W1130 05:02:49.718645    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 05:07:49 minikube kubelet[1505]: W1130 05:07:49.820774    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 05:07:49 minikube kubelet[1505]: W1130 05:07:49.825326    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 05:12:49 minikube kubelet[1505]: W1130 05:12:49.712990    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 05:12:49 minikube kubelet[1505]: W1130 05:12:49.716675    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 05:17:49 minikube kubelet[1505]: W1130 05:17:49.705903    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 05:17:49 minikube kubelet[1505]: W1130 05:17:49.707388    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 05:22:49 minikube kubelet[1505]: W1130 05:22:49.717718    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 05:22:49 minikube kubelet[1505]: W1130 05:22:49.720431    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 05:24:55 minikube kubelet[1505]: I1130 05:24:55.180629    1505 topology_manager.go:215] "Topology Admit Handler" podUID="09b1fc38-477a-47c2-a2a4-74f7c9d8370c" podNamespace="default" podName="esquire-microservice-77565d7689-m6qbb"
Nov 30 05:24:55 minikube kubelet[1505]: E1130 05:24:55.181436    1505 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="9b6b984b-a658-4a1e-842b-1dd007ab063e" containerName="esquire-microservice"
Nov 30 05:24:55 minikube kubelet[1505]: E1130 05:24:55.181465    1505 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="88ca1535-92ab-441b-9cf1-0e0fd6ace0ae" containerName="esquire-microservice"
Nov 30 05:24:55 minikube kubelet[1505]: E1130 05:24:55.181479    1505 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="794683b5-3097-44f6-b6a6-9d1247b9f670" containerName="esquire-microservice"
Nov 30 05:24:55 minikube kubelet[1505]: I1130 05:24:55.182161    1505 memory_manager.go:346] "RemoveStaleState removing state" podUID="794683b5-3097-44f6-b6a6-9d1247b9f670" containerName="esquire-microservice"
Nov 30 05:24:55 minikube kubelet[1505]: I1130 05:24:55.182186    1505 memory_manager.go:346] "RemoveStaleState removing state" podUID="9b6b984b-a658-4a1e-842b-1dd007ab063e" containerName="esquire-microservice"
Nov 30 05:24:55 minikube kubelet[1505]: I1130 05:24:55.182195    1505 memory_manager.go:346] "RemoveStaleState removing state" podUID="88ca1535-92ab-441b-9cf1-0e0fd6ace0ae" containerName="esquire-microservice"
Nov 30 05:24:55 minikube kubelet[1505]: I1130 05:24:55.324280    1505 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vmlq9\" (UniqueName: \"kubernetes.io/projected/09b1fc38-477a-47c2-a2a4-74f7c9d8370c-kube-api-access-vmlq9\") pod \"esquire-microservice-77565d7689-m6qbb\" (UID: \"09b1fc38-477a-47c2-a2a4-74f7c9d8370c\") " pod="default/esquire-microservice-77565d7689-m6qbb"
Nov 30 05:24:56 minikube kubelet[1505]: I1130 05:24:56.133138    1505 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/esquire-microservice-77565d7689-m6qbb" podStartSLOduration=1.132177595 podCreationTimestamp="2023-11-30 05:24:55 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2023-11-30 05:24:56.131601137 +0000 UTC m=+100626.789613224" watchObservedRunningTime="2023-11-30 05:24:56.132177595 +0000 UTC m=+100626.790189849"
Nov 30 05:27:49 minikube kubelet[1505]: W1130 05:27:49.718441    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 05:27:49 minikube kubelet[1505]: W1130 05:27:49.720107    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 05:32:49 minikube kubelet[1505]: W1130 05:32:49.717785    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 05:32:49 minikube kubelet[1505]: W1130 05:32:49.719781    1505 machine.go:65] Cannot read vendor id correctly, set empty.
Nov 30 05:37:49 minikube kubelet[1505]: W1130 05:37:49.722294    1505 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Nov 30 05:37:49 minikube kubelet[1505]: W1130 05:37:49.723885    1505 machine.go:65] Cannot read vendor id correctly, set empty.

* 
* ==> kubernetes-dashboard [346b4c217ab7] <==
* 2023/11/30 05:29:21 [2023-11-30T05:29:21Z] Incoming HTTP/1.1 GET /api/v1/clusterrolebinding?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/11/30 05:29:21 Getting list of all clusterRoleBindings in the cluster
2023/11/30 05:29:21 [2023-11-30T05:29:21Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:29:21 [2023-11-30T05:29:21Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/11/30 05:29:21 [2023-11-30T05:29:21Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:29:21 [2023-11-30T05:29:21Z] Incoming HTTP/1.1 GET /api/v1/namespace?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/11/30 05:29:21 Getting list of namespaces
2023/11/30 05:29:21 [2023-11-30T05:29:21Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:29:25 [2023-11-30T05:29:25Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/11/30 05:29:25 Getting list of namespaces
2023/11/30 05:29:25 [2023-11-30T05:29:25Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:29:26 [2023-11-30T05:29:26Z] Incoming HTTP/1.1 GET /api/v1/namespace?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/11/30 05:29:26 Getting list of namespaces
2023/11/30 05:29:26 [2023-11-30T05:29:26Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:29:27 [2023-11-30T05:29:27Z] Incoming HTTP/1.1 GET /api/v1/namespace?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/11/30 05:29:27 Getting list of namespaces
2023/11/30 05:29:27 [2023-11-30T05:29:27Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/11/30 05:29:27 Getting list of namespaces
2023/11/30 05:29:27 [2023-11-30T05:29:27Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:29:27 [2023-11-30T05:29:27Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:37 [2023-11-30T05:31:37Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/11/30 05:31:37 [2023-11-30T05:31:37Z] Incoming HTTP/1.1 GET /api/v1/namespace?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/11/30 05:31:37 Getting list of namespaces
2023/11/30 05:31:37 Getting list of namespaces
2023/11/30 05:31:37 [2023-11-30T05:31:37Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:37 [2023-11-30T05:31:37Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Incoming HTTP/1.1 GET /api/v1/plugin/config request from 127.0.0.1: 
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Incoming HTTP/1.1 GET /api/v1/settings/global request from 127.0.0.1: 
2023/11/30 05:31:42 Getting application global configuration
2023/11/30 05:31:42 Application configuration {"serverTime":1701322302330}
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Incoming HTTP/1.1 GET /api/v1/settings/pinner request from 127.0.0.1: 
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Incoming HTTP/1.1 GET /api/v1/settings/global request from 127.0.0.1: 
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Incoming HTTP/1.1 GET /api/v1/systembanner request from 127.0.0.1: 
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Incoming HTTP/1.1 GET /api/v1/login/status request from 127.0.0.1: 
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/11/30 05:31:42 Getting list of namespaces
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Incoming HTTP/1.1 GET /api/v1/namespace?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/11/30 05:31:42 Getting list of namespaces
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:42 [2023-11-30T05:31:42Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:47 [2023-11-30T05:31:47Z] Incoming HTTP/1.1 GET /api/v1/namespace?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/11/30 05:31:47 [2023-11-30T05:31:47Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/11/30 05:31:47 Getting list of namespaces
2023/11/30 05:31:47 Getting list of namespaces
2023/11/30 05:31:47 [2023-11-30T05:31:47Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:47 [2023-11-30T05:31:47Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:48 [2023-11-30T05:31:48Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/11/30 05:31:48 Getting list of namespaces
2023/11/30 05:31:48 [2023-11-30T05:31:48Z] Incoming HTTP/1.1 GET /api/v1/namespace?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/11/30 05:31:48 Getting list of namespaces
2023/11/30 05:31:48 [2023-11-30T05:31:48Z] Outcoming response to 127.0.0.1 with 200 status code
2023/11/30 05:31:48 [2023-11-30T05:31:48Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [ab1ef8359cda] <==
* sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x54
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x400001a1a0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x64
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x400001a1a0, 0x1267368, 0x400056f740, 0x1, 0x4000100600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x74
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x400001a1a0, 0x3b9aca00, 0x0, 0x1, 0x4000100600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x88
k8s.io/apimachinery/pkg/util/wait.Until(0x400001a1a0, 0x3b9aca00, 0x4000100600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x48
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x308

goroutine 137 [sync.Cond.Wait, 382 minutes]:
sync.runtime_notifyListWait(0x400013f8d0, 0x3)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0x400013f8c0)
	/usr/local/go/src/sync/cond.go:56 +0xb8
k8s.io/client-go/util/workqueue.(*Type).Get(0x40005162a0, 0x0, 0x0, 0x1c200)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x84
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextClaimWorkItem(0x400041ec80, 0x1298cd0, 0x400037d180, 0x45ed4)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:935 +0x34
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runClaimWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:924
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.2()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x54
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x400001a1c0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x64
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x400001a1c0, 0x1267368, 0x400056f770, 0x1, 0x4000100600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x74
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x400001a1c0, 0x3b9aca00, 0x0, 0x40003c2a01, 0x4000100600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x88
k8s.io/apimachinery/pkg/util/wait.Until(0x400001a1c0, 0x3b9aca00, 0x4000100600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x48
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:880 +0x3c8

goroutine 138 [sync.Cond.Wait, 382 minutes]:
sync.runtime_notifyListWait(0x400013f910, 0x4000000000)
	/usr/local/go/src/runtime/sema.go:513 +0xf8
sync.(*Cond).Wait(0x400013f900)
	/usr/local/go/src/sync/cond.go:56 +0xb8
k8s.io/client-go/util/workqueue.(*Type).Get(0x4000516420, 0x0, 0x0, 0x1c200)
	/Users/medya/go/pkg/mod/k8s.io/client-go@v0.20.5/util/workqueue/queue.go:145 +0x84
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).processNextVolumeWorkItem(0x400041ec80, 0x1298cd0, 0x400037d180, 0xcd70ac)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:990 +0x34
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).runVolumeWorker(...)
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:929
sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1.3()
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x54
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x400001a1e0)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:155 +0x64
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x400001a1e0, 0x1267368, 0x40003202d0, 0x1, 0x4000100600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:156 +0x74
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x400001a1e0, 0x3b9aca00, 0x0, 0x1, 0x4000100600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:133 +0x88
k8s.io/apimachinery/pkg/util/wait.Until(0x400001a1e0, 0x3b9aca00, 0x4000100600)
	/Users/medya/go/pkg/mod/k8s.io/apimachinery@v0.20.5/pkg/util/wait/wait.go:90 +0x48
created by sigs.k8s.io/sig-storage-lib-external-provisioner/v6/controller.(*ProvisionController).Run.func1
	/Users/medya/go/pkg/mod/sigs.k8s.io/sig-storage-lib-external-provisioner/v6@v6.3.0/controller/controller.go:881 +0x308

* 
* ==> storage-provisioner [d209003fc1c7] <==
* I1129 04:28:02.198464       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1129 04:28:02.210220       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1129 04:28:02.210259       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1129 04:28:19.624601       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1129 04:28:19.624994       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"405d7a53-5857-4d63-9073-eec7b51ca1b4", APIVersion:"v1", ResourceVersion:"125265", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_c73923e5-1736-4d2f-9978-5d9953b54e64 became leader
I1129 04:28:19.625092       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_c73923e5-1736-4d2f-9978-5d9953b54e64!
I1129 04:28:19.726733       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_c73923e5-1736-4d2f-9978-5d9953b54e64!

